{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About gazpacho is a simple, fast, and modern web scraping library. The library is stable, actively maintained, and installed with zero dependencies. Install Install with pip at the command line: pip install -U gazpacho Quickstart Give this a try: from gazpacho import get , Soup url = 'https://scrape.world/books' html = get ( url ) soup = Soup ( html ) books = soup . find ( 'div' , { 'class' : 'book-' }, partial = True ) def parse ( book ): name = book . find ( 'h4' ) . text price = float ( book . find ( 'p' ) . text [ 1 :] . split ( ' ' )[ 0 ]) return name , price [ parse ( book ) for book in books ] Tutorial Import Import gazpacho following the convention: from gazpacho import get , Soup get Use the get function to download raw HTML: url = 'https://scrape.world/soup' html = get ( url ) print ( html [: 50 ]) # '<!DOCTYPE html>\\n<html lang=\"en\">\\n <head>\\n <met' Adjust get requests with optional params and headers: get ( url = 'https://httpbin.org/anything' , params = { 'foo' : 'bar' , 'bar' : 'baz' }, headers = { 'User-Agent' : 'gazpacho' } ) Soup Use the Soup wrapper on raw html to enable parsing: soup = Soup ( html ) Soup objects can alternatively be initialized with the .get classmethod: soup = Soup . get ( url ) .find Use the .find method to target and extract HTML tags: h1 = soup . find ( 'h1' ) print ( h1 ) # <h1 id=\"firstHeading\" class=\"firstHeading\" lang=\"en\">Soup</h1> attrs= Use the attrs argument to isolate tags that contain specific HTML element attributes: soup . find ( 'div' , attrs = { 'class' : 'section-' }) partial= Element attributes are partially matched by default. Turn this off by setting partial to False : soup . find ( 'div' , { 'class' : 'soup' }, partial = False ) mode= Override the mode argument { 'auto', 'first', 'all' } to guarantee return behaviour: print ( soup . find ( 'span' , mode = 'first' )) # <span class=\"navbar-toggler-icon\"></span> len ( soup . find ( 'span' , mode = 'all' )) # 8 dir() Soup objects have html , tag , attrs , and text attributes: dir ( h1 ) # ['attrs', 'find', 'get', 'html', 'strip', 'tag', 'text'] Use them accordingly: print ( h1 . html ) # '<h1 id=\"firstHeading\" class=\"firstHeading\" lang=\"en\">Soup</h1>' print ( h1 . tag ) # h1 print ( h1 . attrs ) # {'id': 'firstHeading', 'class': 'firstHeading', 'lang': 'en'} print ( h1 . text ) # Soup Support If you use gazpacho, consider adding the badge to your project README.md: [![scraper: gazpacho](https://img.shields.io/badge/scraper-gazpacho-C6422C)](https://github.com/maxhumber/gazpacho) Contribute For feature requests or bug reports, please use Github Issues For PRs, please read the CONTRIBUTING.md document","title":"Home"},{"location":"#about","text":"gazpacho is a simple, fast, and modern web scraping library. The library is stable, actively maintained, and installed with zero dependencies.","title":"About"},{"location":"#install","text":"Install with pip at the command line: pip install -U gazpacho","title":"Install"},{"location":"#quickstart","text":"Give this a try: from gazpacho import get , Soup url = 'https://scrape.world/books' html = get ( url ) soup = Soup ( html ) books = soup . find ( 'div' , { 'class' : 'book-' }, partial = True ) def parse ( book ): name = book . find ( 'h4' ) . text price = float ( book . find ( 'p' ) . text [ 1 :] . split ( ' ' )[ 0 ]) return name , price [ parse ( book ) for book in books ]","title":"Quickstart"},{"location":"#tutorial","text":"","title":"Tutorial"},{"location":"#import","text":"Import gazpacho following the convention: from gazpacho import get , Soup","title":"Import"},{"location":"#get","text":"Use the get function to download raw HTML: url = 'https://scrape.world/soup' html = get ( url ) print ( html [: 50 ]) # '<!DOCTYPE html>\\n<html lang=\"en\">\\n <head>\\n <met' Adjust get requests with optional params and headers: get ( url = 'https://httpbin.org/anything' , params = { 'foo' : 'bar' , 'bar' : 'baz' }, headers = { 'User-Agent' : 'gazpacho' } )","title":"get"},{"location":"#soup","text":"Use the Soup wrapper on raw html to enable parsing: soup = Soup ( html ) Soup objects can alternatively be initialized with the .get classmethod: soup = Soup . get ( url )","title":"Soup"},{"location":"#find","text":"Use the .find method to target and extract HTML tags: h1 = soup . find ( 'h1' ) print ( h1 ) # <h1 id=\"firstHeading\" class=\"firstHeading\" lang=\"en\">Soup</h1>","title":".find"},{"location":"#attrs","text":"Use the attrs argument to isolate tags that contain specific HTML element attributes: soup . find ( 'div' , attrs = { 'class' : 'section-' })","title":"attrs="},{"location":"#partial","text":"Element attributes are partially matched by default. Turn this off by setting partial to False : soup . find ( 'div' , { 'class' : 'soup' }, partial = False )","title":"partial="},{"location":"#mode","text":"Override the mode argument { 'auto', 'first', 'all' } to guarantee return behaviour: print ( soup . find ( 'span' , mode = 'first' )) # <span class=\"navbar-toggler-icon\"></span> len ( soup . find ( 'span' , mode = 'all' )) # 8","title":"mode="},{"location":"#dir","text":"Soup objects have html , tag , attrs , and text attributes: dir ( h1 ) # ['attrs', 'find', 'get', 'html', 'strip', 'tag', 'text'] Use them accordingly: print ( h1 . html ) # '<h1 id=\"firstHeading\" class=\"firstHeading\" lang=\"en\">Soup</h1>' print ( h1 . tag ) # h1 print ( h1 . attrs ) # {'id': 'firstHeading', 'class': 'firstHeading', 'lang': 'en'} print ( h1 . text ) # Soup","title":"dir()"},{"location":"#support","text":"If you use gazpacho, consider adding the badge to your project README.md: [![scraper: gazpacho](https://img.shields.io/badge/scraper-gazpacho-C6422C)](https://github.com/maxhumber/gazpacho)","title":"Support"},{"location":"#contribute","text":"For feature requests or bug reports, please use Github Issues For PRs, please read the CONTRIBUTING.md document","title":"Contribute"},{"location":"CHANGELOG/","text":"Install To install the latest version of gazpacho run: pip install -U gazpacho Changelog 1.0 (2020-09-24) Feature: gazpacho is now fully baked with type hints (thanks for the suggestion @ju-sh !) Feature: Soup.get(\"url\") alternative initializer Fixed: .find is now able to capture malformed void tags ( <img /> , vs. <img> ) (thanks for the Issue @mallegrini !) Renamed: .find(..., strict=) is now find(..., partial=) Renamed: .remove_tags is now .strip 0.9.4 (2020-07-07) Feature: automagical json-to-dictionary return behaviour for get Improvement: automatic missing URL protocol inference for get Improvement: condensed HTTPError Exceptions 0.9.3 (2020-04-29) Updated the README (thanks for flagging the lxml error, @koaning !) 0.9.2 (2020-04-21) Fixed find(..., mode='first') to return None and not an IndexError (thanks, @psyonara !) 0.9.1 (2020-02-16) Fixed UnicodeEncodeError lurking beneath get (thanks for the \"Issue\" @mlehotay !) Fixed find method to properly handle non-closing HTML tags 0.9 (2019-11-25) Added the remove_tags method for isolating formatted text in a block of HTML 0.8.1 (2019-10-10) Fixed empty element tag counting within the find method 0.8 (2019-10-07) Added mode argument to the find method to adjust return behaviour (defaults to mode='auto' ) Enabled strict attribute matching for the find method (defaults to strict=False )","title":"Changelog"},{"location":"CHANGELOG/#install","text":"To install the latest version of gazpacho run: pip install -U gazpacho","title":"Install"},{"location":"CHANGELOG/#changelog","text":"","title":"Changelog"},{"location":"CHANGELOG/#10-2020-09-24","text":"Feature: gazpacho is now fully baked with type hints (thanks for the suggestion @ju-sh !) Feature: Soup.get(\"url\") alternative initializer Fixed: .find is now able to capture malformed void tags ( <img /> , vs. <img> ) (thanks for the Issue @mallegrini !) Renamed: .find(..., strict=) is now find(..., partial=) Renamed: .remove_tags is now .strip","title":"1.0 (2020-09-24)"},{"location":"CHANGELOG/#094-2020-07-07","text":"Feature: automagical json-to-dictionary return behaviour for get Improvement: automatic missing URL protocol inference for get Improvement: condensed HTTPError Exceptions","title":"0.9.4 (2020-07-07)"},{"location":"CHANGELOG/#093-2020-04-29","text":"Updated the README (thanks for flagging the lxml error, @koaning !)","title":"0.9.3 (2020-04-29)"},{"location":"CHANGELOG/#092-2020-04-21","text":"Fixed find(..., mode='first') to return None and not an IndexError (thanks, @psyonara !)","title":"0.9.2 (2020-04-21)"},{"location":"CHANGELOG/#091-2020-02-16","text":"Fixed UnicodeEncodeError lurking beneath get (thanks for the \"Issue\" @mlehotay !) Fixed find method to properly handle non-closing HTML tags","title":"0.9.1 (2020-02-16)"},{"location":"CHANGELOG/#09-2019-11-25","text":"Added the remove_tags method for isolating formatted text in a block of HTML","title":"0.9 (2019-11-25)"},{"location":"CHANGELOG/#081-2019-10-10","text":"Fixed empty element tag counting within the find method","title":"0.8.1 (2019-10-10)"},{"location":"CHANGELOG/#08-2019-10-07","text":"Added mode argument to the find method to adjust return behaviour (defaults to mode='auto' ) Enabled strict attribute matching for the find method (defaults to strict=False )","title":"0.8 (2019-10-07)"},{"location":"CONTRIBUTING/","text":"Contributing When contributing to gazpacho , please open an issue before making a change Development environment and steps Install pytest either globally or in a virtualenv: pip install pytest Click on the \"Fork\" button at the top-right of the GitHub page Clone your fork: git clone git@github.com:yourname/gazpacho.git Create a new branch to work on the issue/feature you want Hack out your code. Runs the tests by executing pytest from the command line (tests live in the tests subfolder) Submit a new PR with your code, indicating in the PR which issue/feature it relates to Guidelines Keep in mind that gazpacho does not want to do everything. It is a replacement for BeautifulSoup and requests for most projects but not all projects Always write tests for any change introduced If the change involves new methods, arguments or otherwise modifies the public API, make sure to adjust the README.md If the change is beyond cosmetic, add it to the CHANGELOG.md file and give yourself credit!","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"When contributing to gazpacho , please open an issue before making a change","title":"Contributing"},{"location":"CONTRIBUTING/#development-environment-and-steps","text":"Install pytest either globally or in a virtualenv: pip install pytest Click on the \"Fork\" button at the top-right of the GitHub page Clone your fork: git clone git@github.com:yourname/gazpacho.git Create a new branch to work on the issue/feature you want Hack out your code. Runs the tests by executing pytest from the command line (tests live in the tests subfolder) Submit a new PR with your code, indicating in the PR which issue/feature it relates to","title":"Development environment and steps"},{"location":"CONTRIBUTING/#guidelines","text":"Keep in mind that gazpacho does not want to do everything. It is a replacement for BeautifulSoup and requests for most projects but not all projects Always write tests for any change introduced If the change involves new methods, arguments or otherwise modifies the public API, make sure to adjust the README.md If the change is beyond cosmetic, add it to the CHANGELOG.md file and give yourself credit!","title":"Guidelines"},{"location":"reference/gazpacho/","text":"Module gazpacho View Source from .soup import Soup from .get import get Sub-modules gazpacho.get gazpacho.soup gazpacho.utils","title":"Index"},{"location":"reference/gazpacho/#module-gazpacho","text":"View Source from .soup import Soup from .get import get","title":"Module gazpacho"},{"location":"reference/gazpacho/#sub-modules","text":"gazpacho.get gazpacho.soup gazpacho.utils","title":"Sub-modules"},{"location":"reference/gazpacho/get/","text":"Module gazpacho.get View Source import json from typing import Dict , Optional , Union from urllib.error import HTTPError as UrllibHTTPError from urllib.parse import quote , urlencode , urlsplit , urlunsplit from urllib.request import build_opener class HTTPError ( Exception ): def __init__ ( self , code : int , msg : str ) -> None : self . code = code self . msg = msg def __str__ ( self ): return f \"{self.code} - {self.msg}\" def sanitize ( url : str ) -> str : \"\"\"\\ Sanitize and format a URL Arguments: - url: target page \"\"\" scheme , netloc , path , query , fragment = urlsplit ( url ) if not scheme : scheme , netloc , path , query , fragment = urlsplit ( f \"http://{url}\" ) path = quote ( path ) url = urlunsplit (( scheme , netloc , path , query , fragment )) return url def get ( url : str , params : Optional [ Dict [ str , str ]] = None , headers : Optional [ Dict [ str , str ]] = None , ) -> Union [ str , dict ]: \"\"\"Retrive url contents Params: - url: target page - params: GET request payload - headers: GET request headers Example: ``` get('https://httpbin.org/anything', {'soup': 'gazpacho'}) ``` \"\"\" url = sanitize ( url ) opener = build_opener () if params : url += \"?\" + urlencode ( params ) if headers : for h in headers . items (): opener . addheaders = [ h ] if ( headers and not headers . get ( \"User-Agent\" )) or not headers : UA = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:80.0) Gecko/20100101 Firefox/80.0\" opener . addheaders = [( \"User-Agent\" , UA )] try : with opener . open ( url ) as response : content = response . read () . decode ( \"utf-8\" ) if response . headers . get_content_type () == \"application/json\" : content = json . loads ( content ) except UrllibHTTPError as e : raise HTTPError ( e . code , e . msg ) from None return content Functions get def get ( url : str , params : Union [ Dict [ str , str ], NoneType ] = None , headers : Union [ Dict [ str , str ], NoneType ] = None ) -> Union [ str , dict ] Retrive url contents Params: url: target page params: GET request payload headers: GET request headers Example: get('https://httpbin.org/anything', {'soup': 'gazpacho'}) View Source def get ( url : str , params : Optional [ Dict[str, str ] ] = None , headers : Optional [ Dict[str, str ] ] = None , ) -> Union [ str, dict ] : \"\"\"Retrive url contents Params: - url: target page - params: GET request payload - headers: GET request headers Example: ``` get('https://httpbin.org/anything', {'soup': 'gazpacho'}) ``` \"\"\" url = sanitize ( url ) opener = build_opener () if params : url += \"?\" + urlencode ( params ) if headers : for h in headers . items () : opener . addheaders = [ h ] if ( headers and not headers . get ( \"User-Agent\" )) or not headers : UA = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:80.0) Gecko/20100101 Firefox/80.0\" opener . addheaders = [ (\"User-Agent\", UA) ] try : with opener . open ( url ) as response : content = response . read (). decode ( \"utf-8\" ) if response . headers . get_content_type () == \"application/json\" : content = json . loads ( content ) except UrllibHTTPError as e : raise HTTPError ( e . code , e . msg ) from None return content sanitize def sanitize ( url : str ) -> str Sanitize and format a URL Arguments: url: target page View Source def sanitize ( url : str ) -> str : \"\"\"\\ Sanitize and format a URL Arguments: - url: target page \"\"\" scheme , netloc , path , query , fragment = urlsplit ( url ) if not scheme : scheme , netloc , path , query , fragment = urlsplit ( f \"http://{url}\" ) path = quote ( path ) url = urlunsplit (( scheme , netloc , path , query , fragment )) return url Classes HTTPError class HTTPError ( code : int , msg : str ) Common base class for all non-exit exceptions. View Source class HTTPError ( Exception ): def __init__ ( self , code: int , msg: str ) -> None: self . code = code self . msg = msg def __str__ ( self ): return f \"{self.code} - {self.msg}\" Ancestors (in MRO) builtins.Exception builtins.BaseException Class variables args Methods with_traceback def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"Get"},{"location":"reference/gazpacho/get/#module-gazpachoget","text":"View Source import json from typing import Dict , Optional , Union from urllib.error import HTTPError as UrllibHTTPError from urllib.parse import quote , urlencode , urlsplit , urlunsplit from urllib.request import build_opener class HTTPError ( Exception ): def __init__ ( self , code : int , msg : str ) -> None : self . code = code self . msg = msg def __str__ ( self ): return f \"{self.code} - {self.msg}\" def sanitize ( url : str ) -> str : \"\"\"\\ Sanitize and format a URL Arguments: - url: target page \"\"\" scheme , netloc , path , query , fragment = urlsplit ( url ) if not scheme : scheme , netloc , path , query , fragment = urlsplit ( f \"http://{url}\" ) path = quote ( path ) url = urlunsplit (( scheme , netloc , path , query , fragment )) return url def get ( url : str , params : Optional [ Dict [ str , str ]] = None , headers : Optional [ Dict [ str , str ]] = None , ) -> Union [ str , dict ]: \"\"\"Retrive url contents Params: - url: target page - params: GET request payload - headers: GET request headers Example: ``` get('https://httpbin.org/anything', {'soup': 'gazpacho'}) ``` \"\"\" url = sanitize ( url ) opener = build_opener () if params : url += \"?\" + urlencode ( params ) if headers : for h in headers . items (): opener . addheaders = [ h ] if ( headers and not headers . get ( \"User-Agent\" )) or not headers : UA = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:80.0) Gecko/20100101 Firefox/80.0\" opener . addheaders = [( \"User-Agent\" , UA )] try : with opener . open ( url ) as response : content = response . read () . decode ( \"utf-8\" ) if response . headers . get_content_type () == \"application/json\" : content = json . loads ( content ) except UrllibHTTPError as e : raise HTTPError ( e . code , e . msg ) from None return content","title":"Module gazpacho.get"},{"location":"reference/gazpacho/get/#functions","text":"","title":"Functions"},{"location":"reference/gazpacho/get/#get","text":"def get ( url : str , params : Union [ Dict [ str , str ], NoneType ] = None , headers : Union [ Dict [ str , str ], NoneType ] = None ) -> Union [ str , dict ] Retrive url contents Params: url: target page params: GET request payload headers: GET request headers Example: get('https://httpbin.org/anything', {'soup': 'gazpacho'}) View Source def get ( url : str , params : Optional [ Dict[str, str ] ] = None , headers : Optional [ Dict[str, str ] ] = None , ) -> Union [ str, dict ] : \"\"\"Retrive url contents Params: - url: target page - params: GET request payload - headers: GET request headers Example: ``` get('https://httpbin.org/anything', {'soup': 'gazpacho'}) ``` \"\"\" url = sanitize ( url ) opener = build_opener () if params : url += \"?\" + urlencode ( params ) if headers : for h in headers . items () : opener . addheaders = [ h ] if ( headers and not headers . get ( \"User-Agent\" )) or not headers : UA = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:80.0) Gecko/20100101 Firefox/80.0\" opener . addheaders = [ (\"User-Agent\", UA) ] try : with opener . open ( url ) as response : content = response . read (). decode ( \"utf-8\" ) if response . headers . get_content_type () == \"application/json\" : content = json . loads ( content ) except UrllibHTTPError as e : raise HTTPError ( e . code , e . msg ) from None return content","title":"get"},{"location":"reference/gazpacho/get/#sanitize","text":"def sanitize ( url : str ) -> str Sanitize and format a URL Arguments: url: target page View Source def sanitize ( url : str ) -> str : \"\"\"\\ Sanitize and format a URL Arguments: - url: target page \"\"\" scheme , netloc , path , query , fragment = urlsplit ( url ) if not scheme : scheme , netloc , path , query , fragment = urlsplit ( f \"http://{url}\" ) path = quote ( path ) url = urlunsplit (( scheme , netloc , path , query , fragment )) return url","title":"sanitize"},{"location":"reference/gazpacho/get/#classes","text":"","title":"Classes"},{"location":"reference/gazpacho/get/#httperror","text":"class HTTPError ( code : int , msg : str ) Common base class for all non-exit exceptions. View Source class HTTPError ( Exception ): def __init__ ( self , code: int , msg: str ) -> None: self . code = code self . msg = msg def __str__ ( self ): return f \"{self.code} - {self.msg}\"","title":"HTTPError"},{"location":"reference/gazpacho/get/#ancestors-in-mro","text":"builtins.Exception builtins.BaseException","title":"Ancestors (in MRO)"},{"location":"reference/gazpacho/get/#class-variables","text":"args","title":"Class variables"},{"location":"reference/gazpacho/get/#methods","text":"","title":"Methods"},{"location":"reference/gazpacho/get/#with_traceback","text":"def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"with_traceback"},{"location":"reference/gazpacho/soup/","text":"Module gazpacho.soup View Source from collections import Counter from html.parser import HTMLParser from random import sample import re from typing import Any , Dict , Optional , Tuple , Union , List import warnings from .get import get from .utils import match , recover_html_and_attrs class Soup ( HTMLParser ): \"\"\"\\ HTML Soup Parser Attributes: - html: content to parse - tag: element to match - attrs: element attributes to match - text: inner data Methods: - find: matching content by element tag (and attributes) - strip: brackets, tags, and attributes from inner data - get: alternate initializer Deprecations: - remove_tags: (as of 1.0) use strip Examples: ``` from gazpacho import Soup html = \"<div><p class='a'>1</p><p class='a'>2</p><p class='b'>3</p></div>\" url = \"https://www.gazpacho.xyz\" soup = Soup(html) soup = Soup.get(url) ``` \"\"\" def __init__ ( self , html : Optional [ str ] = None ) -> None : \"\"\"\\ Arguments: - html: content to parse \"\"\" super () . __init__ () self . html = \"\" if not html else html self . tag : Optional [ str ] = None self . attrs : Optional [ Dict [ Any , Any ]] = None self . text : Optional [ str ] = None def __dir__ ( self ): return [ \"attrs\" , \"find\" , \"get\" , \"html\" , \"strip\" , \"tag\" , \"text\" ] def __repr__ ( self ) -> str : return self . html @classmethod def get ( cls , url : str , params : Optional [ Dict [ str , str ]] = None , headers : Optional [ Dict [ str , str ]] = None , ) -> \"Soup\" : \"\"\"\\ Intialize with gazpacho.get \"\"\" html = get ( url , params , headers ) if not isinstance ( html , str ): raise Exception ( f \"Unable to retrieve contents from {url}\" ) return cls ( html ) @property def _active ( self ) -> bool : return sum ( self . counter . values ()) > 0 @staticmethod def _void ( tag : str ) -> bool : return tag in [ \"area\" , \"base\" , \"br\" , \"col\" , \"embed\" , \"hr\" , \"img\" , \"input\" , \"keygen\" , \"link\" , \"meta\" , \"param\" , \"source\" , \"track\" , \"wbr\" , ] def _handle_start ( self , tag : str , attrs : List [ Tuple [ str , Optional [ str ]]]) -> None : html , attrs_dict = recover_html_and_attrs ( tag , attrs ) query_attrs = {} if not self . attrs else self . attrs matching = match ( query_attrs , attrs_dict , partial = self . partial ) if ( tag == self . tag ) and ( matching ) and ( not self . _active ): self . groups . append ( Soup ()) self . groups [ - 1 ] . tag = tag self . groups [ - 1 ] . attrs = attrs_dict self . groups [ - 1 ] . html += html self . counter [ tag ] += 1 return if self . _active : self . groups [ - 1 ] . html += html self . counter [ tag ] += 1 def handle_starttag ( self , tag : str , attrs : List [ Tuple [ str , Optional [ str ]]]) -> None : self . _handle_start ( tag , attrs ) if self . _active : if self . _void ( tag ): self . counter [ tag ] -= 1 def handle_startendtag ( self , tag : str , attrs : List [ Tuple [ str , Optional [ str ]]] ) -> None : self . _handle_start ( tag , attrs ) if self . _active : self . counter [ tag ] -= 1 def handle_data ( self , data : str ) -> None : if self . _active : if self . groups [ - 1 ] . text is None : self . groups [ - 1 ] . text = data . strip () self . groups [ - 1 ] . html += data def handle_endtag ( self , tag : str ) -> None : if self . _active : self . groups [ - 1 ] . html += f \"</{tag}>\" self . counter [ tag ] -= 1 def strip ( self , whitespace : bool = True ) -> str : \"\"\"\\ Strip brackets, tags, and attributes from inner text Arguments: - whitespace: remove extra whitespace characters Example: ``` html = \"<span>AB<b>C</b>D</span>\" soup = Soup(html) soup.find(\"span\").text # AB soup.strip() # ABCD ``` \"\"\" text = re . sub ( \"<[^>]+>\" , \"\" , self . html ) if whitespace : text = \" \" . join ( text . split ()) return text def remove_tags ( self , strip : bool = True ) -> str : \"\"\"\\ Now: .strip() \"\"\" message = \"Marked for removal; use .strip()\" warnings . warn ( message , category = FutureWarning , stacklevel = 2 ) return self . strip ( whitespace = strip ) def find ( self , tag : str , attrs : Optional [ Dict [ str , str ]] = None , * , partial : bool = True , mode : str = \"automatic\" , strict : Optional [ bool ] = None , ) -> Optional [ Union [ List [ \"Soup\" ], \"Soup\" ]]: \"\"\"\\ Return matching HTML elements Arguments: - tag: target element tag - attrs: target element attributes - partial: match on attributes - mode: override return behavior {'auto/automatic', 'all/list', 'first'} Deprecations: - strict: (as of 1.0) use partial= Examples: ``` soup.find('p', {'class': 'a'}) # [<p class=\"a\">1</p>, <p class=\"a\">2</p>] soup.find('p', {'class': 'a'}, mode='first') # <p class=\"a\">1</p> result = soup.find('p', {'class': 'b'}, mode='auto') print(result) # <p class=\"b\">3</p> print(result.text) # 3 ``` \"\"\" self . counter : Counter = Counter () self . groups : List = [] self . tag = tag self . attrs = attrs self . partial = partial if strict is not None : message = \"Marked for removal; use partial=\" warnings . warn ( message , category = FutureWarning , stacklevel = 2 ) partial = not strict self . feed ( self . html ) automatic = [ \"auto\" , \"automatic\" ] all = [ \"all\" , \"list\" ] first = [ \"first\" ] last = [ \"last\" ] # undocumented random = [ \"random\" ] # undocumented if not self . groups : if mode in all : return [] else : return None elif mode in automatic : if len ( self . groups ) == 1 : return self . groups [ 0 ] else : return self . groups elif mode in all : return self . groups elif mode in first : return self . groups [ 0 ] elif mode in last : return self . groups [ - 1 ] elif mode in random : return sample ( self . groups , k = 1 )[ 0 ] else : raise ValueError ( mode ) Classes Soup class Soup ( html : Union [ str , NoneType ] = None ) HTML Soup Parser Attributes: html: content to parse tag: element to match attrs: element attributes to match text: inner data Methods: find: matching content by element tag (and attributes) strip: brackets, tags, and attributes from inner data get: alternate initializer Deprecations: remove_tags: (as of 1.0) use strip Examples: from gazpacho import Soup html = \"<div><p class='a'>1</p><p class='a'>2</p><p class='b'>3</p></div>\" url = \"https://www.gazpacho.xyz\" soup = Soup ( html ) soup = Soup . get ( url ) View Source class Soup ( HTMLParser ): \"\"\"\\ HTML Soup Parser Attributes: - html: content to parse - tag: element to match - attrs: element attributes to match - text: inner data Methods: - find: matching content by element tag (and attributes) - strip: brackets, tags, and attributes from inner data - get: alternate initializer Deprecations: - remove_tags: (as of 1.0) use strip Examples: ``` from gazpacho import Soup html = \"<div><p class='a'>1</p><p class='a'>2</p><p class='b'>3</p></div>\" url = \"https://www.gazpacho.xyz\" soup = Soup(html) soup = Soup.get(url) ``` \"\"\" def __init__ ( self , html : Optional [ str ] = None ) -> None : \"\"\"\\ Arguments: - html: content to parse \"\"\" super () . __init__ () self . html = \"\" if not html else html self . tag : Optional [ str ] = None self . attrs : Optional [ Dict [ Any , Any ]] = None self . text : Optional [ str ] = None def __dir__ ( self ): return [ \"attrs\" , \"find\" , \"get\" , \"html\" , \"strip\" , \"tag\" , \"text\" ] def __repr__ ( self ) -> str : return self . html @classmethod def get ( cls , url : str , params : Optional [ Dict [ str , str ]] = None , headers : Optional [ Dict [ str , str ]] = None , ) -> \"Soup\" : \"\"\"\\ Intialize with gazpacho.get \"\"\" html = get ( url , params , headers ) if not isinstance ( html , str ): raise Exception ( f \"Unable to retrieve contents from {url}\" ) return cls ( html ) @property def _active ( self ) -> bool : return sum ( self . counter . values ()) > 0 @staticmethod def _void ( tag : str ) -> bool : return tag in [ \"area\" , \"base\" , \"br\" , \"col\" , \"embed\" , \"hr\" , \"img\" , \"input\" , \"keygen\" , \"link\" , \"meta\" , \"param\" , \"source\" , \"track\" , \"wbr\" , ] def _handle_start ( self , tag : str , attrs : List [ Tuple [ str , Optional [ str ]]]) -> None : html , attrs_dict = recover_html_and_attrs ( tag , attrs ) query_attrs = {} if not self . attrs else self . attrs matching = match ( query_attrs , attrs_dict , partial = self . partial ) if ( tag == self . tag ) and ( matching ) and ( not self . _active ): self . groups . append ( Soup ()) self . groups [ - 1 ] . tag = tag self . groups [ - 1 ] . attrs = attrs_dict self . groups [ - 1 ] . html += html self . counter [ tag ] += 1 return if self . _active : self . groups [ - 1 ] . html += html self . counter [ tag ] += 1 def handle_starttag ( self , tag : str , attrs : List [ Tuple [ str , Optional [ str ]]]) -> None : self . _handle_start ( tag , attrs ) if self . _active : if self . _void ( tag ): self . counter [ tag ] -= 1 def handle_startendtag ( self , tag : str , attrs : List [ Tuple [ str , Optional [ str ]]] ) -> None : self . _handle_start ( tag , attrs ) if self . _active : self . counter [ tag ] -= 1 def handle_data ( self , data : str ) -> None : if self . _active : if self . groups [ - 1 ] . text is None : self . groups [ - 1 ] . text = data . strip () self . groups [ - 1 ] . html += data def handle_endtag ( self , tag : str ) -> None : if self . _active : self . groups [ - 1 ] . html += f \"</{tag}>\" self . counter [ tag ] -= 1 def strip ( self , whitespace : bool = True ) -> str : \"\"\"\\ Strip brackets, tags, and attributes from inner text Arguments: - whitespace: remove extra whitespace characters Example: ``` html = \"<span>AB<b>C</b>D</span>\" soup = Soup(html) soup.find(\"span\").text # AB soup.strip() # ABCD ``` \"\"\" text = re . sub ( \"<[^>]+>\" , \"\" , self . html ) if whitespace : text = \" \" . join ( text . split ()) return text def remove_tags ( self , strip : bool = True ) -> str : \"\"\"\\ Now: .strip() \"\"\" message = \"Marked for removal; use .strip()\" warnings . warn ( message , category = FutureWarning , stacklevel = 2 ) return self . strip ( whitespace = strip ) def find ( self , tag : str , attrs : Optional [ Dict [ str , str ]] = None , * , partial : bool = True , mode : str = \"automatic\" , strict : Optional [ bool ] = None , ) -> Optional [ Union [ List [ \"Soup\" ], \"Soup\" ]]: \"\"\"\\ Return matching HTML elements Arguments: - tag: target element tag - attrs: target element attributes - partial: match on attributes - mode: override return behavior {'auto/automatic', 'all/list', 'first'} Deprecations: - strict: (as of 1.0) use partial= Examples: ``` soup.find('p', {'class': 'a'}) # [<p class=\"a\">1</p>, <p class=\"a\">2</p>] soup.find('p', {'class': 'a'}, mode='first') # <p class=\"a\">1</p> result = soup.find('p', {'class': 'b'}, mode='auto') print(result) # <p class=\"b\">3</p> print(result.text) # 3 ``` \"\"\" self . counter : Counter = Counter () self . groups : List = [] self . tag = tag self . attrs = attrs self . partial = partial if strict is not None : message = \"Marked for removal; use partial=\" warnings . warn ( message , category = FutureWarning , stacklevel = 2 ) partial = not strict self . feed ( self . html ) automatic = [ \"auto\" , \"automatic\" ] all = [ \"all\" , \"list\" ] first = [ \"first\" ] last = [ \"last\" ] # undocumented random = [ \"random\" ] # undocumented if not self . groups : if mode in all : return [] else : return None elif mode in automatic : if len ( self . groups ) == 1 : return self . groups [ 0 ] else : return self . groups elif mode in all : return self . groups elif mode in first : return self . groups [ 0 ] elif mode in last : return self . groups [ - 1 ] elif mode in random : return sample ( self . groups , k = 1 )[ 0 ] else : raise ValueError ( mode ) Ancestors (in MRO) html.parser.HTMLParser _markupbase.ParserBase Class variables CDATA_CONTENT_ELEMENTS Static methods get def get ( url : str , params : Union [ Dict [ str , str ], NoneType ] = None , headers : Union [ Dict [ str , str ], NoneType ] = None ) -> 'Soup' Intialize with gazpacho.get View Source @classmethod def get ( cls , url : str , params : Optional [ Dict[str, str ] ] = None , headers : Optional [ Dict[str, str ] ] = None , ) -> \"Soup\" : \"\"\"\\ Intialize with gazpacho.get \"\"\" html = get ( url , params , headers ) if not isinstance ( html , str ) : raise Exception ( f \"Unable to retrieve contents from {url}\" ) return cls ( html ) Methods check_for_whole_start_tag def check_for_whole_start_tag ( self , i ) View Source def check_for_whole_start_tag ( self , i ): rawdata = self . rawdata m = locatestarttagend_tolerant . match ( rawdata , i ) if m : j = m . end () next = rawdata [ j : j + 1 ] if next == \">\" : return j + 1 if next == \"/\" : if rawdata . startswith ( \"/>\" , j ): return j + 2 if rawdata . startswith ( \"/\" , j ): # buffer boundary return - 1 # else bogus input if j > i : return j else : return i + 1 if next == \"\" : # end of input return - 1 if next in ( \"abcdefghijklmnopqrstuvwxyz=/\" \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" ): # end of input in or before attribute value , or we have the # '/' from a '/>' ending return - 1 if j > i : return j else : return i + 1 raise AssertionError ( \"we should not get here!\" ) clear_cdata_mode def clear_cdata_mode ( self ) View Source def clear_cdata_mode ( self ): self . interesting = interesting_normal self . cdata_elem = None close def close ( self ) Handle any buffered data. View Source def close ( self ): \"\"\"Handle any buffered data.\"\"\" self . goahead ( 1 ) error def error ( self , message ) View Source def error ( self , message ): raise NotImplementedError ( \"subclasses of ParserBase must override error()\" ) feed def feed ( self , data ) Feed data to the parser. Call this as often as you want, with as little or as much text as you want (may include '\\n'). View Source def feed ( self , data ): r \"\"\"Feed data to the parser. Call this as often as you want, with as little or as much text as you want (may include '\\n'). \"\"\" self . rawdata = self . rawdata + data self . goahead ( 0 ) find def find ( self , tag : str , attrs : Union [ Dict [ str , str ], NoneType ] = None , * , partial : bool = True , mode : str = 'automatic' , strict : Union [ bool , NoneType ] = None ) -> Union [ List [ ForwardRef ( 'Soup' )], ForwardRef ( 'Soup' ), NoneType ] Return matching HTML elements Arguments: tag: target element tag attrs: target element attributes partial: match on attributes mode: override return behavior {'auto/automatic', 'all/list', 'first'} Deprecations: strict: (as of 1.0) use partial= Examples: soup.find('p', {'class': 'a'}) # [ <p class= \"a\" > 1 </p> , <p class= \"a\" > 2 </p> ] soup.find('p', {'class': 'a'}, mode='first') # <p class= \"a\" > 1 </p> result = soup.find('p', {'class': 'b'}, mode='auto') print(result) # <p class= \"b\" > 3 </p> print(result.text) # 3 View Source def find ( self , tag : str , attrs : Optional [ Dict[str, str ] ] = None , * , partial : bool = True , mode : str = \"automatic\" , strict : Optional [ bool ] = None , ) -> Optional [ Union[List[\"Soup\" ] , \"Soup\" ]]: \"\"\"\\ Return matching HTML elements Arguments: - tag: target element tag - attrs: target element attributes - partial: match on attributes - mode: override return behavior {'auto/automatic', 'all/list', 'first'} Deprecations: - strict: (as of 1.0) use partial= Examples: ``` soup.find('p', {'class': 'a'}) # [<p class=\" a \">1</p>, <p class=\" a \">2</p>] soup.find('p', {'class': 'a'}, mode='first') # <p class=\" a \">1</p> result = soup.find('p', {'class': 'b'}, mode='auto') print(result) # <p class=\" b \">3</p> print(result.text) # 3 ``` \"\"\" self . counter : Counter = Counter () self . groups : List = [] self . tag = tag self . attrs = attrs self . partial = partial if strict is not None : message = \"Marked for removal; use partial=\" warnings . warn ( message , category = FutureWarning , stacklevel = 2 ) partial = not strict self . feed ( self . html ) automatic = [ \"auto\", \"automatic\" ] all = [ \"all\", \"list\" ] first = [ \"first\" ] last = [ \"last\" ] # undocumented random = [ \"random\" ] # undocumented if not self . groups : if mode in all : return [] else : return None elif mode in automatic : if len ( self . groups ) == 1 : return self . groups [ 0 ] else : return self . groups elif mode in all : return self . groups elif mode in first : return self . groups [ 0 ] elif mode in last : return self . groups [ -1 ] elif mode in random : return sample ( self . groups , k = 1 ) [ 0 ] else : raise ValueError ( mode ) get_starttag_text def get_starttag_text ( self ) Return full source of start tag: '<...>'. View Source def get_starttag_text ( self ): \"\"\"Return full source of start tag: '<...>'.\"\"\" return self . __starttag_text getpos def getpos ( self ) Return current line number and offset. View Source def getpos ( self ): \"\"\"Return current line number and offset.\"\"\" return self . lineno , self . offset goahead def goahead ( self , end ) View Source def goahead ( self , end ) : rawdata = self . rawdata i = 0 n = len ( rawdata ) while i < n : if self . convert_charrefs and not self . cdata_elem: j = rawdata . find ( '<' , i ) if j < 0 : # if we can't find the next <, either we are at the end # or there's more text incoming . If the latter is True , # we can't pass the text to handle_data in case we have # a charref cut in half at end. Try to determine if # this is the case before proceeding by looking for an # & near the end and see if it's followed by a space or ;. amppos = rawdata . rfind ( '&' , max ( i , n - 34 )) if ( amppos >= 0 and not re . compile ( r'[\\s;]' ). search ( rawdata , amppos )) : break # wait till we get all the text j = n else : match = self . interesting . search ( rawdata , i ) # < or & if match : j = match . start () else : if self . cdata_elem: break j = n if i < j : if self . convert_charrefs and not self . cdata_elem: self . handle_data ( unescape ( rawdata [ i : j ])) else : self . handle_data ( rawdata [ i : j ]) i = self . updatepos ( i , j ) if i == n : break startswith = rawdata . startswith if startswith ( '<' , i ) : if starttagopen . match ( rawdata , i ) : # < + letter k = self . parse_starttag ( i ) elif startswith ( \"</\" , i ) : k = self . parse_endtag ( i ) elif startswith ( \"<!--\" , i ) : k = self . parse_comment ( i ) elif startswith ( \"<?\" , i ) : k = self . parse_pi ( i ) elif startswith ( \"<!\" , i ) : k = self . parse_html_declaration ( i ) elif ( i + 1 ) < n : self . handle_data ( \"<\" ) k = i + 1 else : break if k < 0 : if not end : break k = rawdata . find ( '>' , i + 1 ) if k < 0 : k = rawdata . find ( '<' , i + 1 ) if k < 0 : k = i + 1 else : k += 1 if self . convert_charrefs and not self . cdata_elem: self . handle_data ( unescape ( rawdata [ i : k ])) else : self . handle_data ( rawdata [ i : k ]) i = self . updatepos ( i , k ) elif startswith ( \"&#\" , i ) : match = charref . match ( rawdata , i ) if match : name = match . group ()[ 2 :- 1 ] self . handle_charref ( name ) k = match . end () if not startswith ( ';' , k - 1 ) : k = k - 1 i = self . updatepos ( i , k ) continue else : if \";\" in rawdata [ i :] : # bail by consuming &# self . handle_data ( rawdata [ i : i + 2 ]) i = self . updatepos ( i , i + 2 ) break elif startswith ( '&' , i ) : match = entityref . match ( rawdata , i ) if match : name = match . group ( 1 ) self . handle_entityref ( name ) k = match . end () if not startswith ( ';' , k - 1 ) : k = k - 1 i = self . updatepos ( i , k ) continue match = incomplete . match ( rawdata , i ) if match : # match . group () will contain at least 2 chars if end and match . group () == rawdata [ i :] : k = match . end () if k <= i : k = n i = self . updatepos ( i , i + 1 ) # incomplete break elif ( i + 1 ) < n : # not the end of the buffer , and can ' t be confused # with some other construct self . handle_data ( \"&\" ) i = self . updatepos ( i , i + 1 ) else : break else : assert 0 , \"interesting.search() lied\" # end while if end and i < n and not self . cdata_elem: if self . convert_charrefs and not self . cdata_elem: self . handle_data ( unescape ( rawdata [ i : n ])) else : self . handle_data ( rawdata [ i : n ]) i = self . updatepos ( i , n ) self . rawdata = rawdata [ i :] handle_charref def handle_charref ( self , name ) View Source def handle_charref ( self , name ): pass handle_comment def handle_comment ( self , data ) View Source def handle_comment ( self , data ): pass handle_data def handle_data ( self , data : str ) -> None View Source def handle_data ( self , data : str ) -> None : if self . _active : if self . groups [ - 1 ]. text is None : self . groups [ - 1 ]. text = data . strip () self . groups [ - 1 ]. html += data handle_decl def handle_decl ( self , decl ) View Source def handle_decl ( self , decl ): pass handle_endtag def handle_endtag ( self , tag : str ) -> None View Source def handle_endtag ( self , tag : str ) -> None : if self . _active : self . groups [ -1 ] . html += f \"</{tag}>\" self . counter [ tag ] -= 1 handle_entityref def handle_entityref ( self , name ) View Source def handle_entityref ( self , name ): pass handle_pi def handle_pi ( self , data ) View Source def handle_pi ( self , data ): pass handle_startendtag def handle_startendtag ( self , tag : str , attrs : List [ Tuple [ str , Union [ str , NoneType ]]] ) -> None View Source def handle_startendtag ( self , tag : str , attrs : List [ Tuple[str, Optional[str ] ]] ) -> None : self . _handle_start ( tag , attrs ) if self . _active : self . counter [ tag ] -= 1 handle_starttag def handle_starttag ( self , tag : str , attrs : List [ Tuple [ str , Union [ str , NoneType ]]] ) -> None View Source def handle_starttag ( self , tag : str , attrs : List [ Tuple[str, Optional[str ] ]] ) -> None : self . _handle_start ( tag , attrs ) if self . _active : if self . _void ( tag ) : self . counter [ tag ] -= 1 parse_bogus_comment def parse_bogus_comment ( self , i , report = 1 ) View Source def parse_bogus_comment ( self , i , report = 1 ): rawdata = self . rawdata assert rawdata [ i : i + 2 ] in ( '<!' , '</' ), ( 'unexpected call to ' 'parse_comment()' ) pos = rawdata . find ( '>' , i + 2 ) if pos == - 1 : return - 1 if report : self . handle_comment ( rawdata [ i + 2 : pos ]) return pos + 1 parse_comment def parse_comment ( self , i , report = 1 ) View Source def parse_comment ( self , i , report = 1 ): rawdata = self . rawdata if rawdata [ i : i + 4 ] != '<!--' : self . error ( 'unexpected call to parse_comment()' ) match = _commentclose . search ( rawdata , i + 4 ) if not match : return - 1 if report : j = match . start ( 0 ) self . handle_comment ( rawdata [ i + 4 : j ]) return match . end ( 0 ) parse_declaration def parse_declaration ( self , i ) View Source def parse_declaration ( self , i ) : # This is some sort of declaration ; in \"HTML as # deployed,\" this should only be the document type # declaration (\"<!DOCTYPE html...>\"). # ISO 8879:1986, however, has more complex # declaration syntax for elements in <!...>, including: # --comment-- # [marked section] # name in the following list: ENTITY, DOCTYPE, ELEMENT, # ATTLIST, NOTATION, SHORTREF, USEMAP, # LINKTYPE, LINK, IDLINK, USELINK, SYSTEM rawdata = self . rawdata j = i + 2 assert rawdata [ i : j ] == \"<!\" , \"unexpected call to parse_declaration\" if rawdata [ j : j + 1 ] == \">\" : # the empty comment <!> return j + 1 if rawdata [ j : j + 1 ] in ( \"-\" , \"\" ) : # Start of comment followed by buffer boundary, # or just a buffer boundary. return - 1 # A simple, practical version could look like: ((name|stringlit) S*) + '>' n = len ( rawdata ) if rawdata [ j : j + 2 ] == ' -- ' : # comment # Locate --.*-- as the body of the comment return self . parse_comment ( i ) elif rawdata [ j ] == '[' : # marked section # Locate [statusWord [...arbitrary SGML...]] as the body of the marked section # Where statusWord is one of TEMP, CDATA, IGNORE, INCLUDE, RCDATA # Note that this is extended by Microsoft Office \"Save as Web\" function # to include [if...] and [endif]. return self . parse_marked_section ( i ) else : # all other declaration elements decltype , j = self . _scan_name ( j , i ) if j < 0 : return j if decltype == \"doctype\" : self . _decl_otherchars = '' while j < n : c = rawdata [ j ] if c == \">\" : # end of declaration syntax data = rawdata [ i + 2 : j ] if decltype == \"doctype\" : self . handle_decl ( data ) else : # According to the HTML5 specs sections \"8.2.4.44 Bogus # comment state\" and \"8.2.4.45 Markup declaration open # state\", a comment token should be emitted. # Calling unknown_decl provides more flexibility though. self . unknown_decl ( data ) return j + 1 if c in \" \\\" '\" : m = _declstringlit_match ( rawdata , j ) if not m : return - 1 # incomplete j = m . end () elif c in \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\" : name , j = self . _scan_name ( j , i ) elif c in self . _decl_otherchars : j = j + 1 elif c == \"[\" : # this could be handled in a separate doctype parser if decltype == \"doctype\" : j = self . _parse_doctype_subset ( j + 1 , i ) elif decltype in { \"attlist\" , \"linktype\" , \"link\" , \"element\" } : # must tolerate []'d groups in a content model in an element declaration # also in data attribute specifications of attlist declaration # also link type declaration subsets in linktype declarations # also link attribute specification lists in link declarations self . error ( \"unsupported '[' char in %s declaration\" % decltype ) else : self . error ( \"unexpected '[' char in declaration\" ) else : self . error ( \"unexpected %r char in declaration\" % rawdata [ j ]) if j < 0 : return j return - 1 # incomplete parse_endtag def parse_endtag ( self , i ) View Source def parse_endtag(self, i): rawdata = self.rawdata assert rawdata[i:i+2] == \" < /\", \"unexpected call to parse_endtag\" match = endendtag.search(rawdata, i+1) # > if not match: return -1 gtpos = match.end() match = endtagfind.match(rawdata, i) # < / + tag + > if not match: if self.cdata_elem is not None: self.handle_data(rawdata[i:gtpos]) return gtpos # find the name: w3.org/TR/html5/tokenization.html#tag-name-state namematch = tagfind_tolerant.match(rawdata, i+2) if not namematch: # w3.org/TR/html5/tokenization.html#end-tag-open-state if rawdata[i:i+3] == ' < />': return i+3 else: return self.parse_bogus_comment(i) tagname = namematch.group(1).lower() # consume and ignore other stuff between the name and the > # Note: this is not 100% correct, since we might have things like # < /tag attr=\">\">, but looking for > after tha name should cover # most of the cases and is much simpler gtpos = rawdata.find('>', namematch.end()) self.handle_endtag(tagname) return gtpos+1 elem = match.group(1).lower() # script or style if self.cdata_elem is not None: if elem != self.cdata_elem: self.handle_data(rawdata[i:gtpos]) return gtpos self.handle_endtag(elem) self.clear_cdata_mode() return gtpos parse_html_declaration def parse_html_declaration ( self , i ) View Source def parse_html_declaration ( self , i ): rawdata = self . rawdata assert rawdata [ i : i + 2 ] == '<!' , ( 'unexpected call to ' 'parse_html_declaration()' ) if rawdata [ i : i + 4 ] == '<!--' : # this case is actually already handled in goahead () return self . parse_comment ( i ) elif rawdata [ i : i + 3 ] == '<![' : return self . parse_marked_section ( i ) elif rawdata [ i : i + 9 ]. lower () == '<!doctype' : # find the closing > gtpos = rawdata . find ( '>' , i + 9 ) if gtpos == - 1 : return - 1 self . handle_decl ( rawdata [ i + 2 : gtpos ]) return gtpos + 1 else : return self . parse_bogus_comment ( i ) parse_marked_section def parse_marked_section ( self , i , report = 1 ) View Source def parse_marked_section ( self , i , report = 1 ): rawdata = self . rawdata assert rawdata [ i : i + 3 ] == '<![' , \"unexpected call to parse_marked_section()\" sectName , j = self . _scan_name ( i + 3 , i ) if j < 0 : return j if sectName in { \"temp\" , \"cdata\" , \"ignore\" , \"include\" , \"rcdata\" } : # look for standard ]] > ending match = _markedsectionclose . search ( rawdata , i + 3 ) elif sectName in { \"if\" , \"else\" , \"endif\" } : # look for MS Office ] > ending match = _msmarkedsectionclose . search ( rawdata , i + 3 ) else : self . error ( 'unknown status keyword %r in marked section' % rawdata [ i + 3 : j ]) if not match : return - 1 if report : j = match . start ( 0 ) self . unknown_decl ( rawdata [ i + 3 : j ]) return match . end ( 0 ) parse_pi def parse_pi ( self , i ) View Source def parse_pi ( self , i ) : rawdata = self . rawdata assert rawdata [ i : i + 2 ] == ' <? ', ' unexpected call to parse_pi () ' match = piclose . search ( rawdata , i + 2 ) # > if not match : return - 1 j = match . start () self . handle_pi ( rawdata [ i + 2 : j ]) j = match . end () return j parse_starttag def parse_starttag ( self , i ) View Source def parse_starttag ( self , i ) : self . __ starttag_text = None endpos = self . check_for_whole_start_tag ( i ) if endpos < 0 : return endpos rawdata = self . rawdata self . __ starttag_text = rawdata [ i : endpos ] # Now parse the data between i + 1 and j into a tag and attrs attrs = [] match = tagfind_tolerant . match ( rawdata , i + 1 ) assert match , 'unexpected call to parse_starttag()' k = match . end () self . lasttag = tag = match . group ( 1 ). lower () while k < endpos : m = attrfind_tolerant . match ( rawdata , k ) if not m : break attrname , rest , attrvalue = m . group ( 1 , 2 , 3 ) if not rest : attrvalue = None elif attrvalue [ : 1 ] == '\\'' == attrvalue[-1:] or \\ attrvalue[:1] == ' \"' == attrvalue[-1:]: attrvalue = attrvalue[1:-1] if attrvalue: attrvalue = unescape(attrvalue) attrs.append((attrname.lower(), attrvalue)) k = m.end() end = rawdata[k:endpos].strip() if end not in (\" > \", \" /> \"): lineno, offset = self.getpos() if \" \\n \" in self.__starttag_text: lineno = lineno + self.__starttag_text.count(\" \\n \") offset = len(self.__starttag_text) \\ - self.__starttag_text.rfind(\" \\n \") else: offset = offset + len(self.__starttag_text) self.handle_data(rawdata[i:endpos]) return endpos if end.endswith('/>'): # XHTML-style empty tag: <span attr=\" value \" /> self . handle_startendtag ( tag , attrs ) else : self . handle_starttag ( tag , attrs ) if tag in self . CDATA_CONTENT_ELEMENTS : self . set_cdata_mode ( tag ) return endpos remove_tags def remove_tags ( self , strip : bool = True ) -> str Now: .strip() View Source def remove_tags ( self , strip : bool = True ) -> str : \"\"\"\\ Now: .strip() \"\"\" message = \"Marked for removal; use .strip()\" warnings . warn ( message , category = FutureWarning , stacklevel = 2 ) return self . strip ( whitespace = strip ) reset def reset ( self ) Reset this instance. Loses all unprocessed data. View Source def reset ( self ): \"\"\"Reset this instance. Loses all unprocessed data.\"\"\" self . rawdata = '' self . lasttag = '???' self . interesting = interesting_normal self . cdata_elem = None _markupbase . ParserBase . reset ( self ) set_cdata_mode def set_cdata_mode ( self , elem ) View Source def set_cdata_mode ( self , elem ): self . cdata_elem = elem . lower () self . interesting = re . compile ( r '</\\s*%s\\s*>' % self . cdata_elem , re . I ) strip def strip ( self , whitespace : bool = True ) -> str Strip brackets, tags, and attributes from inner text Arguments: whitespace: remove extra whitespace characters Example: html = \" <span> AB <b> C </b> D </span> \" soup = Soup(html) soup.find(\"span\").text # AB soup.strip() # ABCD View Source def strip(self, whitespace: bool = True) -> str: \"\"\"\\ Strip brackets, tags, and attributes from inner text Arguments: - whitespace: remove extra whitespace characters Example: ``` html = \" <span> AB <b> C </b> D </span> \" soup = Soup(html) soup.find(\"span\").text # AB soup.strip() # ABCD ``` \"\"\" text = re.sub(\" < [^>]+>\", \"\", self.html) if whitespace: text = \" \".join(text.split()) return text unescape def unescape ( self , s ) View Source def unescape ( self , s ): warnings . warn ( 'The unescape method is deprecated and will be removed ' 'in 3.5, use html.unescape() instead.' , DeprecationWarning , stacklevel = 2 ) return unescape ( s ) unknown_decl def unknown_decl ( self , data ) View Source def unknown_decl ( self , data ): pass updatepos def updatepos ( self , i , j ) View Source def updatepos ( self , i , j ): if i >= j : return j rawdata = self . rawdata nlines = rawdata . count ( \"\\n\" , i , j ) if nlines : self . lineno = self . lineno + nlines pos = rawdata . rindex ( \"\\n\" , i , j ) # Should not fail self . offset = j - ( pos + 1 ) else : self . offset = self . offset + j - i return j","title":"Soup"},{"location":"reference/gazpacho/soup/#module-gazpachosoup","text":"View Source from collections import Counter from html.parser import HTMLParser from random import sample import re from typing import Any , Dict , Optional , Tuple , Union , List import warnings from .get import get from .utils import match , recover_html_and_attrs class Soup ( HTMLParser ): \"\"\"\\ HTML Soup Parser Attributes: - html: content to parse - tag: element to match - attrs: element attributes to match - text: inner data Methods: - find: matching content by element tag (and attributes) - strip: brackets, tags, and attributes from inner data - get: alternate initializer Deprecations: - remove_tags: (as of 1.0) use strip Examples: ``` from gazpacho import Soup html = \"<div><p class='a'>1</p><p class='a'>2</p><p class='b'>3</p></div>\" url = \"https://www.gazpacho.xyz\" soup = Soup(html) soup = Soup.get(url) ``` \"\"\" def __init__ ( self , html : Optional [ str ] = None ) -> None : \"\"\"\\ Arguments: - html: content to parse \"\"\" super () . __init__ () self . html = \"\" if not html else html self . tag : Optional [ str ] = None self . attrs : Optional [ Dict [ Any , Any ]] = None self . text : Optional [ str ] = None def __dir__ ( self ): return [ \"attrs\" , \"find\" , \"get\" , \"html\" , \"strip\" , \"tag\" , \"text\" ] def __repr__ ( self ) -> str : return self . html @classmethod def get ( cls , url : str , params : Optional [ Dict [ str , str ]] = None , headers : Optional [ Dict [ str , str ]] = None , ) -> \"Soup\" : \"\"\"\\ Intialize with gazpacho.get \"\"\" html = get ( url , params , headers ) if not isinstance ( html , str ): raise Exception ( f \"Unable to retrieve contents from {url}\" ) return cls ( html ) @property def _active ( self ) -> bool : return sum ( self . counter . values ()) > 0 @staticmethod def _void ( tag : str ) -> bool : return tag in [ \"area\" , \"base\" , \"br\" , \"col\" , \"embed\" , \"hr\" , \"img\" , \"input\" , \"keygen\" , \"link\" , \"meta\" , \"param\" , \"source\" , \"track\" , \"wbr\" , ] def _handle_start ( self , tag : str , attrs : List [ Tuple [ str , Optional [ str ]]]) -> None : html , attrs_dict = recover_html_and_attrs ( tag , attrs ) query_attrs = {} if not self . attrs else self . attrs matching = match ( query_attrs , attrs_dict , partial = self . partial ) if ( tag == self . tag ) and ( matching ) and ( not self . _active ): self . groups . append ( Soup ()) self . groups [ - 1 ] . tag = tag self . groups [ - 1 ] . attrs = attrs_dict self . groups [ - 1 ] . html += html self . counter [ tag ] += 1 return if self . _active : self . groups [ - 1 ] . html += html self . counter [ tag ] += 1 def handle_starttag ( self , tag : str , attrs : List [ Tuple [ str , Optional [ str ]]]) -> None : self . _handle_start ( tag , attrs ) if self . _active : if self . _void ( tag ): self . counter [ tag ] -= 1 def handle_startendtag ( self , tag : str , attrs : List [ Tuple [ str , Optional [ str ]]] ) -> None : self . _handle_start ( tag , attrs ) if self . _active : self . counter [ tag ] -= 1 def handle_data ( self , data : str ) -> None : if self . _active : if self . groups [ - 1 ] . text is None : self . groups [ - 1 ] . text = data . strip () self . groups [ - 1 ] . html += data def handle_endtag ( self , tag : str ) -> None : if self . _active : self . groups [ - 1 ] . html += f \"</{tag}>\" self . counter [ tag ] -= 1 def strip ( self , whitespace : bool = True ) -> str : \"\"\"\\ Strip brackets, tags, and attributes from inner text Arguments: - whitespace: remove extra whitespace characters Example: ``` html = \"<span>AB<b>C</b>D</span>\" soup = Soup(html) soup.find(\"span\").text # AB soup.strip() # ABCD ``` \"\"\" text = re . sub ( \"<[^>]+>\" , \"\" , self . html ) if whitespace : text = \" \" . join ( text . split ()) return text def remove_tags ( self , strip : bool = True ) -> str : \"\"\"\\ Now: .strip() \"\"\" message = \"Marked for removal; use .strip()\" warnings . warn ( message , category = FutureWarning , stacklevel = 2 ) return self . strip ( whitespace = strip ) def find ( self , tag : str , attrs : Optional [ Dict [ str , str ]] = None , * , partial : bool = True , mode : str = \"automatic\" , strict : Optional [ bool ] = None , ) -> Optional [ Union [ List [ \"Soup\" ], \"Soup\" ]]: \"\"\"\\ Return matching HTML elements Arguments: - tag: target element tag - attrs: target element attributes - partial: match on attributes - mode: override return behavior {'auto/automatic', 'all/list', 'first'} Deprecations: - strict: (as of 1.0) use partial= Examples: ``` soup.find('p', {'class': 'a'}) # [<p class=\"a\">1</p>, <p class=\"a\">2</p>] soup.find('p', {'class': 'a'}, mode='first') # <p class=\"a\">1</p> result = soup.find('p', {'class': 'b'}, mode='auto') print(result) # <p class=\"b\">3</p> print(result.text) # 3 ``` \"\"\" self . counter : Counter = Counter () self . groups : List = [] self . tag = tag self . attrs = attrs self . partial = partial if strict is not None : message = \"Marked for removal; use partial=\" warnings . warn ( message , category = FutureWarning , stacklevel = 2 ) partial = not strict self . feed ( self . html ) automatic = [ \"auto\" , \"automatic\" ] all = [ \"all\" , \"list\" ] first = [ \"first\" ] last = [ \"last\" ] # undocumented random = [ \"random\" ] # undocumented if not self . groups : if mode in all : return [] else : return None elif mode in automatic : if len ( self . groups ) == 1 : return self . groups [ 0 ] else : return self . groups elif mode in all : return self . groups elif mode in first : return self . groups [ 0 ] elif mode in last : return self . groups [ - 1 ] elif mode in random : return sample ( self . groups , k = 1 )[ 0 ] else : raise ValueError ( mode )","title":"Module gazpacho.soup"},{"location":"reference/gazpacho/soup/#classes","text":"","title":"Classes"},{"location":"reference/gazpacho/soup/#soup","text":"class Soup ( html : Union [ str , NoneType ] = None ) HTML Soup Parser Attributes: html: content to parse tag: element to match attrs: element attributes to match text: inner data Methods: find: matching content by element tag (and attributes) strip: brackets, tags, and attributes from inner data get: alternate initializer Deprecations: remove_tags: (as of 1.0) use strip Examples: from gazpacho import Soup html = \"<div><p class='a'>1</p><p class='a'>2</p><p class='b'>3</p></div>\" url = \"https://www.gazpacho.xyz\" soup = Soup ( html ) soup = Soup . get ( url ) View Source class Soup ( HTMLParser ): \"\"\"\\ HTML Soup Parser Attributes: - html: content to parse - tag: element to match - attrs: element attributes to match - text: inner data Methods: - find: matching content by element tag (and attributes) - strip: brackets, tags, and attributes from inner data - get: alternate initializer Deprecations: - remove_tags: (as of 1.0) use strip Examples: ``` from gazpacho import Soup html = \"<div><p class='a'>1</p><p class='a'>2</p><p class='b'>3</p></div>\" url = \"https://www.gazpacho.xyz\" soup = Soup(html) soup = Soup.get(url) ``` \"\"\" def __init__ ( self , html : Optional [ str ] = None ) -> None : \"\"\"\\ Arguments: - html: content to parse \"\"\" super () . __init__ () self . html = \"\" if not html else html self . tag : Optional [ str ] = None self . attrs : Optional [ Dict [ Any , Any ]] = None self . text : Optional [ str ] = None def __dir__ ( self ): return [ \"attrs\" , \"find\" , \"get\" , \"html\" , \"strip\" , \"tag\" , \"text\" ] def __repr__ ( self ) -> str : return self . html @classmethod def get ( cls , url : str , params : Optional [ Dict [ str , str ]] = None , headers : Optional [ Dict [ str , str ]] = None , ) -> \"Soup\" : \"\"\"\\ Intialize with gazpacho.get \"\"\" html = get ( url , params , headers ) if not isinstance ( html , str ): raise Exception ( f \"Unable to retrieve contents from {url}\" ) return cls ( html ) @property def _active ( self ) -> bool : return sum ( self . counter . values ()) > 0 @staticmethod def _void ( tag : str ) -> bool : return tag in [ \"area\" , \"base\" , \"br\" , \"col\" , \"embed\" , \"hr\" , \"img\" , \"input\" , \"keygen\" , \"link\" , \"meta\" , \"param\" , \"source\" , \"track\" , \"wbr\" , ] def _handle_start ( self , tag : str , attrs : List [ Tuple [ str , Optional [ str ]]]) -> None : html , attrs_dict = recover_html_and_attrs ( tag , attrs ) query_attrs = {} if not self . attrs else self . attrs matching = match ( query_attrs , attrs_dict , partial = self . partial ) if ( tag == self . tag ) and ( matching ) and ( not self . _active ): self . groups . append ( Soup ()) self . groups [ - 1 ] . tag = tag self . groups [ - 1 ] . attrs = attrs_dict self . groups [ - 1 ] . html += html self . counter [ tag ] += 1 return if self . _active : self . groups [ - 1 ] . html += html self . counter [ tag ] += 1 def handle_starttag ( self , tag : str , attrs : List [ Tuple [ str , Optional [ str ]]]) -> None : self . _handle_start ( tag , attrs ) if self . _active : if self . _void ( tag ): self . counter [ tag ] -= 1 def handle_startendtag ( self , tag : str , attrs : List [ Tuple [ str , Optional [ str ]]] ) -> None : self . _handle_start ( tag , attrs ) if self . _active : self . counter [ tag ] -= 1 def handle_data ( self , data : str ) -> None : if self . _active : if self . groups [ - 1 ] . text is None : self . groups [ - 1 ] . text = data . strip () self . groups [ - 1 ] . html += data def handle_endtag ( self , tag : str ) -> None : if self . _active : self . groups [ - 1 ] . html += f \"</{tag}>\" self . counter [ tag ] -= 1 def strip ( self , whitespace : bool = True ) -> str : \"\"\"\\ Strip brackets, tags, and attributes from inner text Arguments: - whitespace: remove extra whitespace characters Example: ``` html = \"<span>AB<b>C</b>D</span>\" soup = Soup(html) soup.find(\"span\").text # AB soup.strip() # ABCD ``` \"\"\" text = re . sub ( \"<[^>]+>\" , \"\" , self . html ) if whitespace : text = \" \" . join ( text . split ()) return text def remove_tags ( self , strip : bool = True ) -> str : \"\"\"\\ Now: .strip() \"\"\" message = \"Marked for removal; use .strip()\" warnings . warn ( message , category = FutureWarning , stacklevel = 2 ) return self . strip ( whitespace = strip ) def find ( self , tag : str , attrs : Optional [ Dict [ str , str ]] = None , * , partial : bool = True , mode : str = \"automatic\" , strict : Optional [ bool ] = None , ) -> Optional [ Union [ List [ \"Soup\" ], \"Soup\" ]]: \"\"\"\\ Return matching HTML elements Arguments: - tag: target element tag - attrs: target element attributes - partial: match on attributes - mode: override return behavior {'auto/automatic', 'all/list', 'first'} Deprecations: - strict: (as of 1.0) use partial= Examples: ``` soup.find('p', {'class': 'a'}) # [<p class=\"a\">1</p>, <p class=\"a\">2</p>] soup.find('p', {'class': 'a'}, mode='first') # <p class=\"a\">1</p> result = soup.find('p', {'class': 'b'}, mode='auto') print(result) # <p class=\"b\">3</p> print(result.text) # 3 ``` \"\"\" self . counter : Counter = Counter () self . groups : List = [] self . tag = tag self . attrs = attrs self . partial = partial if strict is not None : message = \"Marked for removal; use partial=\" warnings . warn ( message , category = FutureWarning , stacklevel = 2 ) partial = not strict self . feed ( self . html ) automatic = [ \"auto\" , \"automatic\" ] all = [ \"all\" , \"list\" ] first = [ \"first\" ] last = [ \"last\" ] # undocumented random = [ \"random\" ] # undocumented if not self . groups : if mode in all : return [] else : return None elif mode in automatic : if len ( self . groups ) == 1 : return self . groups [ 0 ] else : return self . groups elif mode in all : return self . groups elif mode in first : return self . groups [ 0 ] elif mode in last : return self . groups [ - 1 ] elif mode in random : return sample ( self . groups , k = 1 )[ 0 ] else : raise ValueError ( mode )","title":"Soup"},{"location":"reference/gazpacho/soup/#ancestors-in-mro","text":"html.parser.HTMLParser _markupbase.ParserBase","title":"Ancestors (in MRO)"},{"location":"reference/gazpacho/soup/#class-variables","text":"CDATA_CONTENT_ELEMENTS","title":"Class variables"},{"location":"reference/gazpacho/soup/#static-methods","text":"","title":"Static methods"},{"location":"reference/gazpacho/soup/#get","text":"def get ( url : str , params : Union [ Dict [ str , str ], NoneType ] = None , headers : Union [ Dict [ str , str ], NoneType ] = None ) -> 'Soup' Intialize with gazpacho.get View Source @classmethod def get ( cls , url : str , params : Optional [ Dict[str, str ] ] = None , headers : Optional [ Dict[str, str ] ] = None , ) -> \"Soup\" : \"\"\"\\ Intialize with gazpacho.get \"\"\" html = get ( url , params , headers ) if not isinstance ( html , str ) : raise Exception ( f \"Unable to retrieve contents from {url}\" ) return cls ( html )","title":"get"},{"location":"reference/gazpacho/soup/#methods","text":"","title":"Methods"},{"location":"reference/gazpacho/soup/#check_for_whole_start_tag","text":"def check_for_whole_start_tag ( self , i ) View Source def check_for_whole_start_tag ( self , i ): rawdata = self . rawdata m = locatestarttagend_tolerant . match ( rawdata , i ) if m : j = m . end () next = rawdata [ j : j + 1 ] if next == \">\" : return j + 1 if next == \"/\" : if rawdata . startswith ( \"/>\" , j ): return j + 2 if rawdata . startswith ( \"/\" , j ): # buffer boundary return - 1 # else bogus input if j > i : return j else : return i + 1 if next == \"\" : # end of input return - 1 if next in ( \"abcdefghijklmnopqrstuvwxyz=/\" \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" ): # end of input in or before attribute value , or we have the # '/' from a '/>' ending return - 1 if j > i : return j else : return i + 1 raise AssertionError ( \"we should not get here!\" )","title":"check_for_whole_start_tag"},{"location":"reference/gazpacho/soup/#clear_cdata_mode","text":"def clear_cdata_mode ( self ) View Source def clear_cdata_mode ( self ): self . interesting = interesting_normal self . cdata_elem = None","title":"clear_cdata_mode"},{"location":"reference/gazpacho/soup/#close","text":"def close ( self ) Handle any buffered data. View Source def close ( self ): \"\"\"Handle any buffered data.\"\"\" self . goahead ( 1 )","title":"close"},{"location":"reference/gazpacho/soup/#error","text":"def error ( self , message ) View Source def error ( self , message ): raise NotImplementedError ( \"subclasses of ParserBase must override error()\" )","title":"error"},{"location":"reference/gazpacho/soup/#feed","text":"def feed ( self , data ) Feed data to the parser. Call this as often as you want, with as little or as much text as you want (may include '\\n'). View Source def feed ( self , data ): r \"\"\"Feed data to the parser. Call this as often as you want, with as little or as much text as you want (may include '\\n'). \"\"\" self . rawdata = self . rawdata + data self . goahead ( 0 )","title":"feed"},{"location":"reference/gazpacho/soup/#find","text":"def find ( self , tag : str , attrs : Union [ Dict [ str , str ], NoneType ] = None , * , partial : bool = True , mode : str = 'automatic' , strict : Union [ bool , NoneType ] = None ) -> Union [ List [ ForwardRef ( 'Soup' )], ForwardRef ( 'Soup' ), NoneType ] Return matching HTML elements Arguments: tag: target element tag attrs: target element attributes partial: match on attributes mode: override return behavior {'auto/automatic', 'all/list', 'first'} Deprecations: strict: (as of 1.0) use partial= Examples: soup.find('p', {'class': 'a'}) # [ <p class= \"a\" > 1 </p> , <p class= \"a\" > 2 </p> ] soup.find('p', {'class': 'a'}, mode='first') # <p class= \"a\" > 1 </p> result = soup.find('p', {'class': 'b'}, mode='auto') print(result) # <p class= \"b\" > 3 </p> print(result.text) # 3 View Source def find ( self , tag : str , attrs : Optional [ Dict[str, str ] ] = None , * , partial : bool = True , mode : str = \"automatic\" , strict : Optional [ bool ] = None , ) -> Optional [ Union[List[\"Soup\" ] , \"Soup\" ]]: \"\"\"\\ Return matching HTML elements Arguments: - tag: target element tag - attrs: target element attributes - partial: match on attributes - mode: override return behavior {'auto/automatic', 'all/list', 'first'} Deprecations: - strict: (as of 1.0) use partial= Examples: ``` soup.find('p', {'class': 'a'}) # [<p class=\" a \">1</p>, <p class=\" a \">2</p>] soup.find('p', {'class': 'a'}, mode='first') # <p class=\" a \">1</p> result = soup.find('p', {'class': 'b'}, mode='auto') print(result) # <p class=\" b \">3</p> print(result.text) # 3 ``` \"\"\" self . counter : Counter = Counter () self . groups : List = [] self . tag = tag self . attrs = attrs self . partial = partial if strict is not None : message = \"Marked for removal; use partial=\" warnings . warn ( message , category = FutureWarning , stacklevel = 2 ) partial = not strict self . feed ( self . html ) automatic = [ \"auto\", \"automatic\" ] all = [ \"all\", \"list\" ] first = [ \"first\" ] last = [ \"last\" ] # undocumented random = [ \"random\" ] # undocumented if not self . groups : if mode in all : return [] else : return None elif mode in automatic : if len ( self . groups ) == 1 : return self . groups [ 0 ] else : return self . groups elif mode in all : return self . groups elif mode in first : return self . groups [ 0 ] elif mode in last : return self . groups [ -1 ] elif mode in random : return sample ( self . groups , k = 1 ) [ 0 ] else : raise ValueError ( mode )","title":"find"},{"location":"reference/gazpacho/soup/#get_starttag_text","text":"def get_starttag_text ( self ) Return full source of start tag: '<...>'. View Source def get_starttag_text ( self ): \"\"\"Return full source of start tag: '<...>'.\"\"\" return self . __starttag_text","title":"get_starttag_text"},{"location":"reference/gazpacho/soup/#getpos","text":"def getpos ( self ) Return current line number and offset. View Source def getpos ( self ): \"\"\"Return current line number and offset.\"\"\" return self . lineno , self . offset","title":"getpos"},{"location":"reference/gazpacho/soup/#goahead","text":"def goahead ( self , end ) View Source def goahead ( self , end ) : rawdata = self . rawdata i = 0 n = len ( rawdata ) while i < n : if self . convert_charrefs and not self . cdata_elem: j = rawdata . find ( '<' , i ) if j < 0 : # if we can't find the next <, either we are at the end # or there's more text incoming . If the latter is True , # we can't pass the text to handle_data in case we have # a charref cut in half at end. Try to determine if # this is the case before proceeding by looking for an # & near the end and see if it's followed by a space or ;. amppos = rawdata . rfind ( '&' , max ( i , n - 34 )) if ( amppos >= 0 and not re . compile ( r'[\\s;]' ). search ( rawdata , amppos )) : break # wait till we get all the text j = n else : match = self . interesting . search ( rawdata , i ) # < or & if match : j = match . start () else : if self . cdata_elem: break j = n if i < j : if self . convert_charrefs and not self . cdata_elem: self . handle_data ( unescape ( rawdata [ i : j ])) else : self . handle_data ( rawdata [ i : j ]) i = self . updatepos ( i , j ) if i == n : break startswith = rawdata . startswith if startswith ( '<' , i ) : if starttagopen . match ( rawdata , i ) : # < + letter k = self . parse_starttag ( i ) elif startswith ( \"</\" , i ) : k = self . parse_endtag ( i ) elif startswith ( \"<!--\" , i ) : k = self . parse_comment ( i ) elif startswith ( \"<?\" , i ) : k = self . parse_pi ( i ) elif startswith ( \"<!\" , i ) : k = self . parse_html_declaration ( i ) elif ( i + 1 ) < n : self . handle_data ( \"<\" ) k = i + 1 else : break if k < 0 : if not end : break k = rawdata . find ( '>' , i + 1 ) if k < 0 : k = rawdata . find ( '<' , i + 1 ) if k < 0 : k = i + 1 else : k += 1 if self . convert_charrefs and not self . cdata_elem: self . handle_data ( unescape ( rawdata [ i : k ])) else : self . handle_data ( rawdata [ i : k ]) i = self . updatepos ( i , k ) elif startswith ( \"&#\" , i ) : match = charref . match ( rawdata , i ) if match : name = match . group ()[ 2 :- 1 ] self . handle_charref ( name ) k = match . end () if not startswith ( ';' , k - 1 ) : k = k - 1 i = self . updatepos ( i , k ) continue else : if \";\" in rawdata [ i :] : # bail by consuming &# self . handle_data ( rawdata [ i : i + 2 ]) i = self . updatepos ( i , i + 2 ) break elif startswith ( '&' , i ) : match = entityref . match ( rawdata , i ) if match : name = match . group ( 1 ) self . handle_entityref ( name ) k = match . end () if not startswith ( ';' , k - 1 ) : k = k - 1 i = self . updatepos ( i , k ) continue match = incomplete . match ( rawdata , i ) if match : # match . group () will contain at least 2 chars if end and match . group () == rawdata [ i :] : k = match . end () if k <= i : k = n i = self . updatepos ( i , i + 1 ) # incomplete break elif ( i + 1 ) < n : # not the end of the buffer , and can ' t be confused # with some other construct self . handle_data ( \"&\" ) i = self . updatepos ( i , i + 1 ) else : break else : assert 0 , \"interesting.search() lied\" # end while if end and i < n and not self . cdata_elem: if self . convert_charrefs and not self . cdata_elem: self . handle_data ( unescape ( rawdata [ i : n ])) else : self . handle_data ( rawdata [ i : n ]) i = self . updatepos ( i , n ) self . rawdata = rawdata [ i :]","title":"goahead"},{"location":"reference/gazpacho/soup/#handle_charref","text":"def handle_charref ( self , name ) View Source def handle_charref ( self , name ): pass","title":"handle_charref"},{"location":"reference/gazpacho/soup/#handle_comment","text":"def handle_comment ( self , data ) View Source def handle_comment ( self , data ): pass","title":"handle_comment"},{"location":"reference/gazpacho/soup/#handle_data","text":"def handle_data ( self , data : str ) -> None View Source def handle_data ( self , data : str ) -> None : if self . _active : if self . groups [ - 1 ]. text is None : self . groups [ - 1 ]. text = data . strip () self . groups [ - 1 ]. html += data","title":"handle_data"},{"location":"reference/gazpacho/soup/#handle_decl","text":"def handle_decl ( self , decl ) View Source def handle_decl ( self , decl ): pass","title":"handle_decl"},{"location":"reference/gazpacho/soup/#handle_endtag","text":"def handle_endtag ( self , tag : str ) -> None View Source def handle_endtag ( self , tag : str ) -> None : if self . _active : self . groups [ -1 ] . html += f \"</{tag}>\" self . counter [ tag ] -= 1","title":"handle_endtag"},{"location":"reference/gazpacho/soup/#handle_entityref","text":"def handle_entityref ( self , name ) View Source def handle_entityref ( self , name ): pass","title":"handle_entityref"},{"location":"reference/gazpacho/soup/#handle_pi","text":"def handle_pi ( self , data ) View Source def handle_pi ( self , data ): pass","title":"handle_pi"},{"location":"reference/gazpacho/soup/#handle_startendtag","text":"def handle_startendtag ( self , tag : str , attrs : List [ Tuple [ str , Union [ str , NoneType ]]] ) -> None View Source def handle_startendtag ( self , tag : str , attrs : List [ Tuple[str, Optional[str ] ]] ) -> None : self . _handle_start ( tag , attrs ) if self . _active : self . counter [ tag ] -= 1","title":"handle_startendtag"},{"location":"reference/gazpacho/soup/#handle_starttag","text":"def handle_starttag ( self , tag : str , attrs : List [ Tuple [ str , Union [ str , NoneType ]]] ) -> None View Source def handle_starttag ( self , tag : str , attrs : List [ Tuple[str, Optional[str ] ]] ) -> None : self . _handle_start ( tag , attrs ) if self . _active : if self . _void ( tag ) : self . counter [ tag ] -= 1","title":"handle_starttag"},{"location":"reference/gazpacho/soup/#parse_bogus_comment","text":"def parse_bogus_comment ( self , i , report = 1 ) View Source def parse_bogus_comment ( self , i , report = 1 ): rawdata = self . rawdata assert rawdata [ i : i + 2 ] in ( '<!' , '</' ), ( 'unexpected call to ' 'parse_comment()' ) pos = rawdata . find ( '>' , i + 2 ) if pos == - 1 : return - 1 if report : self . handle_comment ( rawdata [ i + 2 : pos ]) return pos + 1","title":"parse_bogus_comment"},{"location":"reference/gazpacho/soup/#parse_comment","text":"def parse_comment ( self , i , report = 1 ) View Source def parse_comment ( self , i , report = 1 ): rawdata = self . rawdata if rawdata [ i : i + 4 ] != '<!--' : self . error ( 'unexpected call to parse_comment()' ) match = _commentclose . search ( rawdata , i + 4 ) if not match : return - 1 if report : j = match . start ( 0 ) self . handle_comment ( rawdata [ i + 4 : j ]) return match . end ( 0 )","title":"parse_comment"},{"location":"reference/gazpacho/soup/#parse_declaration","text":"def parse_declaration ( self , i ) View Source def parse_declaration ( self , i ) : # This is some sort of declaration ; in \"HTML as # deployed,\" this should only be the document type # declaration (\"<!DOCTYPE html...>\"). # ISO 8879:1986, however, has more complex # declaration syntax for elements in <!...>, including: # --comment-- # [marked section] # name in the following list: ENTITY, DOCTYPE, ELEMENT, # ATTLIST, NOTATION, SHORTREF, USEMAP, # LINKTYPE, LINK, IDLINK, USELINK, SYSTEM rawdata = self . rawdata j = i + 2 assert rawdata [ i : j ] == \"<!\" , \"unexpected call to parse_declaration\" if rawdata [ j : j + 1 ] == \">\" : # the empty comment <!> return j + 1 if rawdata [ j : j + 1 ] in ( \"-\" , \"\" ) : # Start of comment followed by buffer boundary, # or just a buffer boundary. return - 1 # A simple, practical version could look like: ((name|stringlit) S*) + '>' n = len ( rawdata ) if rawdata [ j : j + 2 ] == ' -- ' : # comment # Locate --.*-- as the body of the comment return self . parse_comment ( i ) elif rawdata [ j ] == '[' : # marked section # Locate [statusWord [...arbitrary SGML...]] as the body of the marked section # Where statusWord is one of TEMP, CDATA, IGNORE, INCLUDE, RCDATA # Note that this is extended by Microsoft Office \"Save as Web\" function # to include [if...] and [endif]. return self . parse_marked_section ( i ) else : # all other declaration elements decltype , j = self . _scan_name ( j , i ) if j < 0 : return j if decltype == \"doctype\" : self . _decl_otherchars = '' while j < n : c = rawdata [ j ] if c == \">\" : # end of declaration syntax data = rawdata [ i + 2 : j ] if decltype == \"doctype\" : self . handle_decl ( data ) else : # According to the HTML5 specs sections \"8.2.4.44 Bogus # comment state\" and \"8.2.4.45 Markup declaration open # state\", a comment token should be emitted. # Calling unknown_decl provides more flexibility though. self . unknown_decl ( data ) return j + 1 if c in \" \\\" '\" : m = _declstringlit_match ( rawdata , j ) if not m : return - 1 # incomplete j = m . end () elif c in \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\" : name , j = self . _scan_name ( j , i ) elif c in self . _decl_otherchars : j = j + 1 elif c == \"[\" : # this could be handled in a separate doctype parser if decltype == \"doctype\" : j = self . _parse_doctype_subset ( j + 1 , i ) elif decltype in { \"attlist\" , \"linktype\" , \"link\" , \"element\" } : # must tolerate []'d groups in a content model in an element declaration # also in data attribute specifications of attlist declaration # also link type declaration subsets in linktype declarations # also link attribute specification lists in link declarations self . error ( \"unsupported '[' char in %s declaration\" % decltype ) else : self . error ( \"unexpected '[' char in declaration\" ) else : self . error ( \"unexpected %r char in declaration\" % rawdata [ j ]) if j < 0 : return j return - 1 # incomplete","title":"parse_declaration"},{"location":"reference/gazpacho/soup/#parse_endtag","text":"def parse_endtag ( self , i ) View Source def parse_endtag(self, i): rawdata = self.rawdata assert rawdata[i:i+2] == \" < /\", \"unexpected call to parse_endtag\" match = endendtag.search(rawdata, i+1) # > if not match: return -1 gtpos = match.end() match = endtagfind.match(rawdata, i) # < / + tag + > if not match: if self.cdata_elem is not None: self.handle_data(rawdata[i:gtpos]) return gtpos # find the name: w3.org/TR/html5/tokenization.html#tag-name-state namematch = tagfind_tolerant.match(rawdata, i+2) if not namematch: # w3.org/TR/html5/tokenization.html#end-tag-open-state if rawdata[i:i+3] == ' < />': return i+3 else: return self.parse_bogus_comment(i) tagname = namematch.group(1).lower() # consume and ignore other stuff between the name and the > # Note: this is not 100% correct, since we might have things like # < /tag attr=\">\">, but looking for > after tha name should cover # most of the cases and is much simpler gtpos = rawdata.find('>', namematch.end()) self.handle_endtag(tagname) return gtpos+1 elem = match.group(1).lower() # script or style if self.cdata_elem is not None: if elem != self.cdata_elem: self.handle_data(rawdata[i:gtpos]) return gtpos self.handle_endtag(elem) self.clear_cdata_mode() return gtpos","title":"parse_endtag"},{"location":"reference/gazpacho/soup/#parse_html_declaration","text":"def parse_html_declaration ( self , i ) View Source def parse_html_declaration ( self , i ): rawdata = self . rawdata assert rawdata [ i : i + 2 ] == '<!' , ( 'unexpected call to ' 'parse_html_declaration()' ) if rawdata [ i : i + 4 ] == '<!--' : # this case is actually already handled in goahead () return self . parse_comment ( i ) elif rawdata [ i : i + 3 ] == '<![' : return self . parse_marked_section ( i ) elif rawdata [ i : i + 9 ]. lower () == '<!doctype' : # find the closing > gtpos = rawdata . find ( '>' , i + 9 ) if gtpos == - 1 : return - 1 self . handle_decl ( rawdata [ i + 2 : gtpos ]) return gtpos + 1 else : return self . parse_bogus_comment ( i )","title":"parse_html_declaration"},{"location":"reference/gazpacho/soup/#parse_marked_section","text":"def parse_marked_section ( self , i , report = 1 ) View Source def parse_marked_section ( self , i , report = 1 ): rawdata = self . rawdata assert rawdata [ i : i + 3 ] == '<![' , \"unexpected call to parse_marked_section()\" sectName , j = self . _scan_name ( i + 3 , i ) if j < 0 : return j if sectName in { \"temp\" , \"cdata\" , \"ignore\" , \"include\" , \"rcdata\" } : # look for standard ]] > ending match = _markedsectionclose . search ( rawdata , i + 3 ) elif sectName in { \"if\" , \"else\" , \"endif\" } : # look for MS Office ] > ending match = _msmarkedsectionclose . search ( rawdata , i + 3 ) else : self . error ( 'unknown status keyword %r in marked section' % rawdata [ i + 3 : j ]) if not match : return - 1 if report : j = match . start ( 0 ) self . unknown_decl ( rawdata [ i + 3 : j ]) return match . end ( 0 )","title":"parse_marked_section"},{"location":"reference/gazpacho/soup/#parse_pi","text":"def parse_pi ( self , i ) View Source def parse_pi ( self , i ) : rawdata = self . rawdata assert rawdata [ i : i + 2 ] == ' <? ', ' unexpected call to parse_pi () ' match = piclose . search ( rawdata , i + 2 ) # > if not match : return - 1 j = match . start () self . handle_pi ( rawdata [ i + 2 : j ]) j = match . end () return j","title":"parse_pi"},{"location":"reference/gazpacho/soup/#parse_starttag","text":"def parse_starttag ( self , i ) View Source def parse_starttag ( self , i ) : self . __ starttag_text = None endpos = self . check_for_whole_start_tag ( i ) if endpos < 0 : return endpos rawdata = self . rawdata self . __ starttag_text = rawdata [ i : endpos ] # Now parse the data between i + 1 and j into a tag and attrs attrs = [] match = tagfind_tolerant . match ( rawdata , i + 1 ) assert match , 'unexpected call to parse_starttag()' k = match . end () self . lasttag = tag = match . group ( 1 ). lower () while k < endpos : m = attrfind_tolerant . match ( rawdata , k ) if not m : break attrname , rest , attrvalue = m . group ( 1 , 2 , 3 ) if not rest : attrvalue = None elif attrvalue [ : 1 ] == '\\'' == attrvalue[-1:] or \\ attrvalue[:1] == ' \"' == attrvalue[-1:]: attrvalue = attrvalue[1:-1] if attrvalue: attrvalue = unescape(attrvalue) attrs.append((attrname.lower(), attrvalue)) k = m.end() end = rawdata[k:endpos].strip() if end not in (\" > \", \" /> \"): lineno, offset = self.getpos() if \" \\n \" in self.__starttag_text: lineno = lineno + self.__starttag_text.count(\" \\n \") offset = len(self.__starttag_text) \\ - self.__starttag_text.rfind(\" \\n \") else: offset = offset + len(self.__starttag_text) self.handle_data(rawdata[i:endpos]) return endpos if end.endswith('/>'): # XHTML-style empty tag: <span attr=\" value \" /> self . handle_startendtag ( tag , attrs ) else : self . handle_starttag ( tag , attrs ) if tag in self . CDATA_CONTENT_ELEMENTS : self . set_cdata_mode ( tag ) return endpos","title":"parse_starttag"},{"location":"reference/gazpacho/soup/#remove_tags","text":"def remove_tags ( self , strip : bool = True ) -> str Now: .strip() View Source def remove_tags ( self , strip : bool = True ) -> str : \"\"\"\\ Now: .strip() \"\"\" message = \"Marked for removal; use .strip()\" warnings . warn ( message , category = FutureWarning , stacklevel = 2 ) return self . strip ( whitespace = strip )","title":"remove_tags"},{"location":"reference/gazpacho/soup/#reset","text":"def reset ( self ) Reset this instance. Loses all unprocessed data. View Source def reset ( self ): \"\"\"Reset this instance. Loses all unprocessed data.\"\"\" self . rawdata = '' self . lasttag = '???' self . interesting = interesting_normal self . cdata_elem = None _markupbase . ParserBase . reset ( self )","title":"reset"},{"location":"reference/gazpacho/soup/#set_cdata_mode","text":"def set_cdata_mode ( self , elem ) View Source def set_cdata_mode ( self , elem ): self . cdata_elem = elem . lower () self . interesting = re . compile ( r '</\\s*%s\\s*>' % self . cdata_elem , re . I )","title":"set_cdata_mode"},{"location":"reference/gazpacho/soup/#strip","text":"def strip ( self , whitespace : bool = True ) -> str Strip brackets, tags, and attributes from inner text Arguments: whitespace: remove extra whitespace characters Example: html = \" <span> AB <b> C </b> D </span> \" soup = Soup(html) soup.find(\"span\").text # AB soup.strip() # ABCD View Source def strip(self, whitespace: bool = True) -> str: \"\"\"\\ Strip brackets, tags, and attributes from inner text Arguments: - whitespace: remove extra whitespace characters Example: ``` html = \" <span> AB <b> C </b> D </span> \" soup = Soup(html) soup.find(\"span\").text # AB soup.strip() # ABCD ``` \"\"\" text = re.sub(\" < [^>]+>\", \"\", self.html) if whitespace: text = \" \".join(text.split()) return text","title":"strip"},{"location":"reference/gazpacho/soup/#unescape","text":"def unescape ( self , s ) View Source def unescape ( self , s ): warnings . warn ( 'The unescape method is deprecated and will be removed ' 'in 3.5, use html.unescape() instead.' , DeprecationWarning , stacklevel = 2 ) return unescape ( s )","title":"unescape"},{"location":"reference/gazpacho/soup/#unknown_decl","text":"def unknown_decl ( self , data ) View Source def unknown_decl ( self , data ): pass","title":"unknown_decl"},{"location":"reference/gazpacho/soup/#updatepos","text":"def updatepos ( self , i , j ) View Source def updatepos ( self , i , j ): if i >= j : return j rawdata = self . rawdata nlines = rawdata . count ( \"\\n\" , i , j ) if nlines : self . lineno = self . lineno + nlines pos = rawdata . rindex ( \"\\n\" , i , j ) # Should not fail self . offset = j - ( pos + 1 ) else : self . offset = self . offset + j - i return j","title":"updatepos"},{"location":"reference/gazpacho/utils/","text":"Module gazpacho.utils View Source from typing import Any , Dict , List , Optional , Tuple , Union def match ( a : Dict [ Any , Any ], b : Dict [ Any , Any ], * , partial : bool = False ) -> bool : \"\"\"Match two dictionaries Arguments: - a: query dict - b: dict to match - partial: allow partial match Examples: ``` a = {'foo': 'bar'} b = {'foo': 'bar baz'} match(a, b, partial=True) # True a = {'foo': 'bar'} b = {'foo': 'bar baz'} match(a, b) # False a = {} b = {'foo': 'bar'} match(a, b) # True a = {} b = {} match(a, b) # True ``` \"\"\" if ( not a ) or ( not a and not b ): return True if a and ( not b ): return False for key , lhs in a . items (): rhs = b . get ( key ) if not rhs : return False if not partial : if lhs == rhs : continue else : return False if lhs in rhs : continue else : return False return True def recover_html_and_attrs ( tag : str , attrs : List [ Tuple [ str , Optional [ str ]]], startendtag : bool = False ) -> Tuple [ str , Dict [ Any , Any ]]: \"\"\"\\ Recover html and attrs from HTMLParser feed Arguments: - tag: element tag - attrs: element attributes - startendtag: if startend tag Example: ``` recover_html_and_attrs('img', [('src', 'example.png')]) # (\"<img src='example.png'>\", {'src': 'example.png'}) ``` \"\"\" if attrs : attrs_dict = dict ( attrs ) attrs_list = [ f '{key}=\"{value}\"' for key , value in attrs_dict . items ()] attrs_str = f ' {\" \".join(attrs_list)}' else : attrs_dict = {} attrs_str = \"\" if startendtag : html = f \"<{tag}{attrs_str} />\" else : html = f \"<{tag}{attrs_str}>\" return html , attrs_dict Functions match def match ( a : Dict [ Any , Any ], b : Dict [ Any , Any ], * , partial : bool = False ) -> bool Match two dictionaries Arguments: a: query dict b: dict to match partial: allow partial match Examples: a = { 'foo' : 'bar' } b = { 'foo' : 'bar baz' } match ( a , b , partial = True ) # True a = { 'foo' : 'bar' } b = { 'foo' : 'bar baz' } match ( a , b ) # False a = {} b = { 'foo' : 'bar' } match ( a , b ) # True a = {} b = {} match ( a , b ) # True View Source def match ( a : Dict [ Any , Any ], b : Dict [ Any , Any ], * , partial : bool = False ) -> bool : \"\"\"Match two dictionaries Arguments: - a: query dict - b: dict to match - partial: allow partial match Examples: ``` a = {'foo': 'bar'} b = {'foo': 'bar baz'} match(a, b, partial=True) # True a = {'foo': 'bar'} b = {'foo': 'bar baz'} match(a, b) # False a = {} b = {'foo': 'bar'} match(a, b) # True a = {} b = {} match(a, b) # True ``` \"\"\" if ( not a ) or ( not a and not b ): return True if a and ( not b ): return False for key , lhs in a . items (): rhs = b . get ( key ) if not rhs : return False if not partial : if lhs == rhs : continue else : return False if lhs in rhs : continue else : return False return True recover_html_and_attrs def recover_html_and_attrs ( tag : str , attrs : List [ Tuple [ str , Union [ str , NoneType ]]], startendtag : bool = False ) -> Tuple [ str , Dict [ Any , Any ]] Recover html and attrs from HTMLParser feed Arguments: tag: element tag attrs: element attributes startendtag: if startend tag Example: recover_html_and_attrs('img', [('src', 'example.png')]) # (\"<img src='example.png'>\", {'src': 'example.png'}) View Source def recover_html_and_attrs ( tag : str , attrs : List [ Tuple[str, Optional[str ] ]] , startendtag : bool = False ) -> Tuple [ str, Dict[Any, Any ] ]: \"\"\"\\ Recover html and attrs from HTMLParser feed Arguments: - tag: element tag - attrs: element attributes - startendtag: if startend tag Example: ``` recover_html_and_attrs('img', [('src', 'example.png')]) # (\" < img src = 'example.png' > \", {'src': 'example.png'}) ``` \"\"\" if attrs : attrs_dict = dict ( attrs ) attrs_list = [ f'{key}=\"{value}\"' for key, value in attrs_dict.items() ] attrs_str = f ' {\" \".join(attrs_list)}' else : attrs_dict = {} attrs_str = \"\" if startendtag : html = f \"<{tag}{attrs_str} />\" else : html = f \"<{tag}{attrs_str}>\" return html , attrs_dict","title":"Utils"},{"location":"reference/gazpacho/utils/#module-gazpachoutils","text":"View Source from typing import Any , Dict , List , Optional , Tuple , Union def match ( a : Dict [ Any , Any ], b : Dict [ Any , Any ], * , partial : bool = False ) -> bool : \"\"\"Match two dictionaries Arguments: - a: query dict - b: dict to match - partial: allow partial match Examples: ``` a = {'foo': 'bar'} b = {'foo': 'bar baz'} match(a, b, partial=True) # True a = {'foo': 'bar'} b = {'foo': 'bar baz'} match(a, b) # False a = {} b = {'foo': 'bar'} match(a, b) # True a = {} b = {} match(a, b) # True ``` \"\"\" if ( not a ) or ( not a and not b ): return True if a and ( not b ): return False for key , lhs in a . items (): rhs = b . get ( key ) if not rhs : return False if not partial : if lhs == rhs : continue else : return False if lhs in rhs : continue else : return False return True def recover_html_and_attrs ( tag : str , attrs : List [ Tuple [ str , Optional [ str ]]], startendtag : bool = False ) -> Tuple [ str , Dict [ Any , Any ]]: \"\"\"\\ Recover html and attrs from HTMLParser feed Arguments: - tag: element tag - attrs: element attributes - startendtag: if startend tag Example: ``` recover_html_and_attrs('img', [('src', 'example.png')]) # (\"<img src='example.png'>\", {'src': 'example.png'}) ``` \"\"\" if attrs : attrs_dict = dict ( attrs ) attrs_list = [ f '{key}=\"{value}\"' for key , value in attrs_dict . items ()] attrs_str = f ' {\" \".join(attrs_list)}' else : attrs_dict = {} attrs_str = \"\" if startendtag : html = f \"<{tag}{attrs_str} />\" else : html = f \"<{tag}{attrs_str}>\" return html , attrs_dict","title":"Module gazpacho.utils"},{"location":"reference/gazpacho/utils/#functions","text":"","title":"Functions"},{"location":"reference/gazpacho/utils/#match","text":"def match ( a : Dict [ Any , Any ], b : Dict [ Any , Any ], * , partial : bool = False ) -> bool Match two dictionaries Arguments: a: query dict b: dict to match partial: allow partial match Examples: a = { 'foo' : 'bar' } b = { 'foo' : 'bar baz' } match ( a , b , partial = True ) # True a = { 'foo' : 'bar' } b = { 'foo' : 'bar baz' } match ( a , b ) # False a = {} b = { 'foo' : 'bar' } match ( a , b ) # True a = {} b = {} match ( a , b ) # True View Source def match ( a : Dict [ Any , Any ], b : Dict [ Any , Any ], * , partial : bool = False ) -> bool : \"\"\"Match two dictionaries Arguments: - a: query dict - b: dict to match - partial: allow partial match Examples: ``` a = {'foo': 'bar'} b = {'foo': 'bar baz'} match(a, b, partial=True) # True a = {'foo': 'bar'} b = {'foo': 'bar baz'} match(a, b) # False a = {} b = {'foo': 'bar'} match(a, b) # True a = {} b = {} match(a, b) # True ``` \"\"\" if ( not a ) or ( not a and not b ): return True if a and ( not b ): return False for key , lhs in a . items (): rhs = b . get ( key ) if not rhs : return False if not partial : if lhs == rhs : continue else : return False if lhs in rhs : continue else : return False return True","title":"match"},{"location":"reference/gazpacho/utils/#recover_html_and_attrs","text":"def recover_html_and_attrs ( tag : str , attrs : List [ Tuple [ str , Union [ str , NoneType ]]], startendtag : bool = False ) -> Tuple [ str , Dict [ Any , Any ]] Recover html and attrs from HTMLParser feed Arguments: tag: element tag attrs: element attributes startendtag: if startend tag Example: recover_html_and_attrs('img', [('src', 'example.png')]) # (\"<img src='example.png'>\", {'src': 'example.png'}) View Source def recover_html_and_attrs ( tag : str , attrs : List [ Tuple[str, Optional[str ] ]] , startendtag : bool = False ) -> Tuple [ str, Dict[Any, Any ] ]: \"\"\"\\ Recover html and attrs from HTMLParser feed Arguments: - tag: element tag - attrs: element attributes - startendtag: if startend tag Example: ``` recover_html_and_attrs('img', [('src', 'example.png')]) # (\" < img src = 'example.png' > \", {'src': 'example.png'}) ``` \"\"\" if attrs : attrs_dict = dict ( attrs ) attrs_list = [ f'{key}=\"{value}\"' for key, value in attrs_dict.items() ] attrs_str = f ' {\" \".join(attrs_list)}' else : attrs_dict = {} attrs_str = \"\" if startendtag : html = f \"<{tag}{attrs_str} />\" else : html = f \"<{tag}{attrs_str}>\" return html , attrs_dict","title":"recover_html_and_attrs"}]}