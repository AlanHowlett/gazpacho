{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About gazpacho is a web scraping library. It replaces requests and BeautifulSoup for most projects. gazpacho is small, simple, fast, and consistent. You should use it! Usage gazpacho is easy to use. To retrieve the contents of a web page use get . And to parse the retrieved contents use Soup . Get The get function retrieves content from a web page: from gazpacho import get url = 'https://en.wikipedia.org/wiki/Gazpacho' html = get ( url ) print ( html [: 50 ]) # <!DOCTYPE html> # <html class=\"client-nojs\" lang=\"en The get function also accepts optional params and headers for any GET request. url = 'https://httpbin.org/anything' get ( url , params = { 'foo' : 'bar' , 'bar' : 'baz' }, headers = { 'User-Agent' : 'gazpacho' }) Soup The Soup object takes an HTML string and turns it into something parsable: from gazpacho import Soup soup = Soup ( html ) str ( soup )[: 50 ] # '<!DOCTYPE html>\\n<html class=\"client-nojs\" lang=\"en' In order to parse an HTML element inside of a Soup object, pass the desired tag and attributes (optional) to the find method: # Original HTML: <span class=\"mw-headline\" id=\"Ingredients_and_preparation\">Ingredients and preparation</span> results = soup . find ( 'span' , { 'class' : 'mw-headline' }) The find method will either return a list of Soup objects if there are multiple elements that satisfy the tag and attribute constraints, or a single Soup object if there's just one: print ( results ) # [<span class=\"mw-headline\" id=\"History\">History</span>, # <span class=\"mw-headline\" id=\"Ingredients_and_preparation\">Ingredients and preparation</span>, # <span class=\"mw-headline\" id=\"Variations\">Variations</span>, # <span class=\"mw-headline\" id=\"In_Spain\">In Spain</span>, # <span class=\"mw-headline\" id=\"Arranque_rote\u00f1o\">Arranque rote\u00f1o</span>, # <span class=\"mw-headline\" id=\"Extremaduran_variations\">Extremaduran variations</span>, # <span class=\"mw-headline\" id=\"La_Mancha_variations\">La Mancha variations</span>, # <span class=\"mw-headline\" id=\"Castilian_variations\">Castilian variations</span>, # <span class=\"mw-headline\" id=\"See_also\">See also</span>, # <span class=\"mw-headline\" id=\"References\">References</span>] The return behaviour of find can be adjusted and made more predictable with the mode argument {'auto', 'first', 'all'} : soup . find ( 'span' , { 'class' : 'mw-headline' }, mode = 'first' ) # <span class=\"mw-headline\" id=\"History\">History</span> Soup objects returned by the find method will have html , tag , attrs , and text attributes: result = results [ 3 ] print ( result . html ) # <span class=\"mw-headline\" id=\"In_Spain\">In Spain</span> print ( result . tag ) # span print ( result . attrs ) # {'class': 'mw-headline', 'id': 'In_Spain'} print ( result . text ) # In Spain And, importantly, returned Soup objects can reimplement the find method! Production gazpacho is production ready. The library currently powers quote , a python wrapper for the Goodreads Quote API. A fully working example of gazpacho in action is available here . Comparison gazpacho is a drop-in replacement for most projects that use requests and BeautifulSoup: import requests from bs4 import BeautifulSoup import pandas as pd url = 'https://www.capfriendly.com/browse/active/2020/salary?p=1' response = requests . get ( url ) soup = BeautifulSoup ( response . text , 'lxml' ) df = pd . read_html ( str ( soup . find ( 'table' )))[ 0 ] print ( df [[ 'PLAYER' , 'TEAM' , 'SALARY' , 'AGE' ]] . head ( 3 )) # PLAYER TEAM SALARY AGE # 0 1. Mitchell Marner TOR $16,000,000 22 # 1 2. John Tavares TOR $15,900,000 28 # 2 3. Auston Matthews TOR $15,900,000 21 Powered by gazpacho: from gazpacho import get , Soup import pandas as pd url = 'https://www.capfriendly.com/browse/active/2020/salary?p=1' response = get ( url ) soup = Soup ( response ) df = pd . read_html ( str ( soup . find ( 'table' )))[ 0 ] print ( df [[ 'PLAYER' , 'TEAM' , 'SALARY' , 'AGE' ]] . head ( 3 )) # PLAYER TEAM SALARY AGE # 0 1. Mitchell Marner TOR $16,000,000 22 # 1 2. John Tavares TOR $15,900,000 28 # 2 3. Auston Matthews TOR $15,900,000 21 Speed gazpacho is fast: from gazpacho import Soup %% timeit soup = Soup ( html ) soup . find ( 'span' , { 'class' : 'mw-headline' }) # 15 ms \u00b1 325 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) gazpacho is often 20-40% faster than BeautifulSoup: from bs4 import BeautifulSoup %% timeit soup = BeautifulSoup ( html , 'lxml' ) soup . find ( 'span' , { 'class' : 'mw-headline' }) # 19.4 ms \u00b1 583 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) And 200-300% faster than requests-html: from requests_html import HTML %% timeit soup = HTML ( html = html ) soup . find ( 'span.mw-headline' ) # 40.1 ms \u00b1 418 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) Installation pip install - U gazpacho Support If you use gazpacho, consider adding the badge to your project README.md: [ ! [ scraper : gazpacho ]( https : // img . shields . io / badge / scraper - gazpacho - C6422C )]( https : // github . com / maxhumber / gazpacho ) Contribute For feature requests or bug reports, please use Github Issues . For PRs, please read the CONTRIBUTING.md document.","title":"Home"},{"location":"#about","text":"gazpacho is a web scraping library. It replaces requests and BeautifulSoup for most projects. gazpacho is small, simple, fast, and consistent. You should use it!","title":"About"},{"location":"#usage","text":"gazpacho is easy to use. To retrieve the contents of a web page use get . And to parse the retrieved contents use Soup .","title":"Usage"},{"location":"#get","text":"The get function retrieves content from a web page: from gazpacho import get url = 'https://en.wikipedia.org/wiki/Gazpacho' html = get ( url ) print ( html [: 50 ]) # <!DOCTYPE html> # <html class=\"client-nojs\" lang=\"en The get function also accepts optional params and headers for any GET request. url = 'https://httpbin.org/anything' get ( url , params = { 'foo' : 'bar' , 'bar' : 'baz' }, headers = { 'User-Agent' : 'gazpacho' })","title":"Get"},{"location":"#soup","text":"The Soup object takes an HTML string and turns it into something parsable: from gazpacho import Soup soup = Soup ( html ) str ( soup )[: 50 ] # '<!DOCTYPE html>\\n<html class=\"client-nojs\" lang=\"en' In order to parse an HTML element inside of a Soup object, pass the desired tag and attributes (optional) to the find method: # Original HTML: <span class=\"mw-headline\" id=\"Ingredients_and_preparation\">Ingredients and preparation</span> results = soup . find ( 'span' , { 'class' : 'mw-headline' }) The find method will either return a list of Soup objects if there are multiple elements that satisfy the tag and attribute constraints, or a single Soup object if there's just one: print ( results ) # [<span class=\"mw-headline\" id=\"History\">History</span>, # <span class=\"mw-headline\" id=\"Ingredients_and_preparation\">Ingredients and preparation</span>, # <span class=\"mw-headline\" id=\"Variations\">Variations</span>, # <span class=\"mw-headline\" id=\"In_Spain\">In Spain</span>, # <span class=\"mw-headline\" id=\"Arranque_rote\u00f1o\">Arranque rote\u00f1o</span>, # <span class=\"mw-headline\" id=\"Extremaduran_variations\">Extremaduran variations</span>, # <span class=\"mw-headline\" id=\"La_Mancha_variations\">La Mancha variations</span>, # <span class=\"mw-headline\" id=\"Castilian_variations\">Castilian variations</span>, # <span class=\"mw-headline\" id=\"See_also\">See also</span>, # <span class=\"mw-headline\" id=\"References\">References</span>] The return behaviour of find can be adjusted and made more predictable with the mode argument {'auto', 'first', 'all'} : soup . find ( 'span' , { 'class' : 'mw-headline' }, mode = 'first' ) # <span class=\"mw-headline\" id=\"History\">History</span> Soup objects returned by the find method will have html , tag , attrs , and text attributes: result = results [ 3 ] print ( result . html ) # <span class=\"mw-headline\" id=\"In_Spain\">In Spain</span> print ( result . tag ) # span print ( result . attrs ) # {'class': 'mw-headline', 'id': 'In_Spain'} print ( result . text ) # In Spain And, importantly, returned Soup objects can reimplement the find method!","title":"Soup"},{"location":"#production","text":"gazpacho is production ready. The library currently powers quote , a python wrapper for the Goodreads Quote API. A fully working example of gazpacho in action is available here .","title":"Production"},{"location":"#comparison","text":"gazpacho is a drop-in replacement for most projects that use requests and BeautifulSoup: import requests from bs4 import BeautifulSoup import pandas as pd url = 'https://www.capfriendly.com/browse/active/2020/salary?p=1' response = requests . get ( url ) soup = BeautifulSoup ( response . text , 'lxml' ) df = pd . read_html ( str ( soup . find ( 'table' )))[ 0 ] print ( df [[ 'PLAYER' , 'TEAM' , 'SALARY' , 'AGE' ]] . head ( 3 )) # PLAYER TEAM SALARY AGE # 0 1. Mitchell Marner TOR $16,000,000 22 # 1 2. John Tavares TOR $15,900,000 28 # 2 3. Auston Matthews TOR $15,900,000 21 Powered by gazpacho: from gazpacho import get , Soup import pandas as pd url = 'https://www.capfriendly.com/browse/active/2020/salary?p=1' response = get ( url ) soup = Soup ( response ) df = pd . read_html ( str ( soup . find ( 'table' )))[ 0 ] print ( df [[ 'PLAYER' , 'TEAM' , 'SALARY' , 'AGE' ]] . head ( 3 )) # PLAYER TEAM SALARY AGE # 0 1. Mitchell Marner TOR $16,000,000 22 # 1 2. John Tavares TOR $15,900,000 28 # 2 3. Auston Matthews TOR $15,900,000 21","title":"Comparison"},{"location":"#speed","text":"gazpacho is fast: from gazpacho import Soup %% timeit soup = Soup ( html ) soup . find ( 'span' , { 'class' : 'mw-headline' }) # 15 ms \u00b1 325 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) gazpacho is often 20-40% faster than BeautifulSoup: from bs4 import BeautifulSoup %% timeit soup = BeautifulSoup ( html , 'lxml' ) soup . find ( 'span' , { 'class' : 'mw-headline' }) # 19.4 ms \u00b1 583 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) And 200-300% faster than requests-html: from requests_html import HTML %% timeit soup = HTML ( html = html ) soup . find ( 'span.mw-headline' ) # 40.1 ms \u00b1 418 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)","title":"Speed"},{"location":"#installation","text":"pip install - U gazpacho","title":"Installation"},{"location":"#support","text":"If you use gazpacho, consider adding the badge to your project README.md: [ ! [ scraper : gazpacho ]( https : // img . shields . io / badge / scraper - gazpacho - C6422C )]( https : // github . com / maxhumber / gazpacho )","title":"Support"},{"location":"#contribute","text":"For feature requests or bug reports, please use Github Issues . For PRs, please read the CONTRIBUTING.md document.","title":"Contribute"},{"location":"CHANGELOG/","text":"Install To install the latest version of gazpacho run: pip install - U gazpacho Changelog 0.9.2 (2020-04-21) Fixed find(..., mode='first') to return None and not an IndexError (thanks, psyonara !) 0.9.1 (2020-02-16) Fixed UnicodeEncodeError lurking beneath get (thanks for the \"Issue\" mlehotay !) Fixed find method to properly handle non-closing HTML tags 0.9 (2019-11-25) Added the remove_tags method for isolating formatted text in a block of HTML 0.8.1 (2019-10-10) Fixed empty element tag counting within the find method 0.8 (2019-10-07) Added mode argument to the find method to adjust return behaviour (defaults to mode='auto' ) Enabled strict attribute matching for the find method (defaults to strict=False )","title":"Changelog"},{"location":"CHANGELOG/#install","text":"To install the latest version of gazpacho run: pip install - U gazpacho","title":"Install"},{"location":"CHANGELOG/#changelog","text":"","title":"Changelog"},{"location":"CHANGELOG/#092-2020-04-21","text":"Fixed find(..., mode='first') to return None and not an IndexError (thanks, psyonara !)","title":"0.9.2 (2020-04-21)"},{"location":"CHANGELOG/#091-2020-02-16","text":"Fixed UnicodeEncodeError lurking beneath get (thanks for the \"Issue\" mlehotay !) Fixed find method to properly handle non-closing HTML tags","title":"0.9.1 (2020-02-16)"},{"location":"CHANGELOG/#09-2019-11-25","text":"Added the remove_tags method for isolating formatted text in a block of HTML","title":"0.9 (2019-11-25)"},{"location":"CHANGELOG/#081-2019-10-10","text":"Fixed empty element tag counting within the find method","title":"0.8.1 (2019-10-10)"},{"location":"CHANGELOG/#08-2019-10-07","text":"Added mode argument to the find method to adjust return behaviour (defaults to mode='auto' ) Enabled strict attribute matching for the find method (defaults to strict=False )","title":"0.8 (2019-10-07)"},{"location":"CONTRIBUTING/","text":"Contributing When contributing to gazpacho , please open an issue before making a change Development environment and steps Install pytest either globally or in a virtualenv: pip install pytest Click on the \"Fork\" button at the top-right of the GitHub page Clone your fork: git clone git@github.com:yourname/gazpacho.git Create a new branch to work on the issue/feature you want Hack out your code. Runs the tests by executing pytest from the command line (tests live in the tests subfolder) Submit a new PR with your code, indicating in the PR which issue/feature it relates to Guidelines Keep in mind that gazpacho does not want to do everything. It is a replacement for BeautifulSoup and requests for most projects but not all projects Always write tests for any change introduced If the change involves new methods, arguments or otherwise modifies the public API, make sure to adjust the README.md If the change is beyond cosmetic, add it to the CHANGELOG.md file and give yourself credit!","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"When contributing to gazpacho , please open an issue before making a change","title":"Contributing"},{"location":"CONTRIBUTING/#development-environment-and-steps","text":"Install pytest either globally or in a virtualenv: pip install pytest Click on the \"Fork\" button at the top-right of the GitHub page Clone your fork: git clone git@github.com:yourname/gazpacho.git Create a new branch to work on the issue/feature you want Hack out your code. Runs the tests by executing pytest from the command line (tests live in the tests subfolder) Submit a new PR with your code, indicating in the PR which issue/feature it relates to","title":"Development environment and steps"},{"location":"CONTRIBUTING/#guidelines","text":"Keep in mind that gazpacho does not want to do everything. It is a replacement for BeautifulSoup and requests for most projects but not all projects Always write tests for any change introduced If the change involves new methods, arguments or otherwise modifies the public API, make sure to adjust the README.md If the change is beyond cosmetic, add it to the CHANGELOG.md file and give yourself credit!","title":"Guidelines"},{"location":"reference/gazpacho/","text":"Module gazpacho View Source from .get import get from .soup import Soup from .utils import match Sub-modules gazpacho.get gazpacho.soup gazpacho.utils","title":"Index"},{"location":"reference/gazpacho/#module-gazpacho","text":"View Source from .get import get from .soup import Soup from .utils import match","title":"Module gazpacho"},{"location":"reference/gazpacho/#sub-modules","text":"gazpacho.get gazpacho.soup gazpacho.utils","title":"Sub-modules"},{"location":"reference/gazpacho/get/","text":"Module gazpacho.get View Source from urllib.parse import quote , urlencode , urlsplit , urlunsplit from urllib.request import build_opener def get ( url , params = None , headers = None ): \"\"\"Return the contents from a URL Params: - url (str): Target website URL - params (dict, optional): Param payload to add to the GET request - headers (dict, optional): Headers to add to the GET request Example: ``` get('https://httpbin.org/anything', {'soup': 'gazpacho'}) ``` \"\"\" scheme , netloc , path , query , fragment = urlsplit ( url ) path = quote ( path ) url = urlunsplit (( scheme , netloc , path , query , fragment )) opener = build_opener () if params : url += \"?\" + urlencode ( params ) if headers : for h in headers . items (): opener . addheaders = [ h ] if ( headers and not headers . get ( \"User-Agent\" )) or not headers : UA = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:72.0) Gecko/20100101 Firefox/72.0\" opener . addheaders = [( \"User-Agent\" , UA )] with opener . open ( url ) as f : content = f . read () . decode ( \"utf-8\" ) return content Functions get def get ( url , params = None , headers = None ) Return the contents from a URL Params: url (str): Target website URL params (dict, optional): Param payload to add to the GET request headers (dict, optional): Headers to add to the GET request Example: get ( 'https://httpbin.org/anything' , { 'soup' : 'gazpacho' } ) View Source def get ( url , params = None , headers = None ) : \"\"\" Return the contents from a URL Params : - url ( str ) : Target website URL - params ( dict , optional ) : Param payload to add to the GET request - headers ( dict , optional ) : Headers to add to the GET request Example : ``` get ( ' https://httpbin.org/anything ' , { ' soup ' : ' gazpacho ' } ) ``` \"\"\" scheme , netloc , path , query , fragment = urlsplit ( url ) path = quote ( path ) url = urlunsplit (( scheme , netloc , path , query , fragment )) opener = build_opener () if params : url += \" ? \" + urlencode ( params ) if headers : for h in headers . items () : opener . addheaders = [ h ] if ( headers and not headers . get ( \" User-Agent \" )) or not headers : UA = \" Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:72.0) Gecko/20100101 Firefox/72.0 \" opener . addheaders = [ ( \" User-Agent \" , UA ) ] with opener . open ( url ) as f : content = f . read () . decode ( \" utf-8 \" ) return content","title":"Get"},{"location":"reference/gazpacho/get/#module-gazpachoget","text":"View Source from urllib.parse import quote , urlencode , urlsplit , urlunsplit from urllib.request import build_opener def get ( url , params = None , headers = None ): \"\"\"Return the contents from a URL Params: - url (str): Target website URL - params (dict, optional): Param payload to add to the GET request - headers (dict, optional): Headers to add to the GET request Example: ``` get('https://httpbin.org/anything', {'soup': 'gazpacho'}) ``` \"\"\" scheme , netloc , path , query , fragment = urlsplit ( url ) path = quote ( path ) url = urlunsplit (( scheme , netloc , path , query , fragment )) opener = build_opener () if params : url += \"?\" + urlencode ( params ) if headers : for h in headers . items (): opener . addheaders = [ h ] if ( headers and not headers . get ( \"User-Agent\" )) or not headers : UA = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:72.0) Gecko/20100101 Firefox/72.0\" opener . addheaders = [( \"User-Agent\" , UA )] with opener . open ( url ) as f : content = f . read () . decode ( \"utf-8\" ) return content","title":"Module gazpacho.get"},{"location":"reference/gazpacho/get/#functions","text":"","title":"Functions"},{"location":"reference/gazpacho/get/#get","text":"def get ( url , params = None , headers = None ) Return the contents from a URL Params: url (str): Target website URL params (dict, optional): Param payload to add to the GET request headers (dict, optional): Headers to add to the GET request Example: get ( 'https://httpbin.org/anything' , { 'soup' : 'gazpacho' } ) View Source def get ( url , params = None , headers = None ) : \"\"\" Return the contents from a URL Params : - url ( str ) : Target website URL - params ( dict , optional ) : Param payload to add to the GET request - headers ( dict , optional ) : Headers to add to the GET request Example : ``` get ( ' https://httpbin.org/anything ' , { ' soup ' : ' gazpacho ' } ) ``` \"\"\" scheme , netloc , path , query , fragment = urlsplit ( url ) path = quote ( path ) url = urlunsplit (( scheme , netloc , path , query , fragment )) opener = build_opener () if params : url += \" ? \" + urlencode ( params ) if headers : for h in headers . items () : opener . addheaders = [ h ] if ( headers and not headers . get ( \" User-Agent \" )) or not headers : UA = \" Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:72.0) Gecko/20100101 Firefox/72.0 \" opener . addheaders = [ ( \" User-Agent \" , UA ) ] with opener . open ( url ) as f : content = f . read () . decode ( \" utf-8 \" ) return content","title":"get"},{"location":"reference/gazpacho/soup/","text":"Module gazpacho.soup View Source from html.parser import HTMLParser import re from .utils import match , html_starttag_and_attrs class Soup ( HTMLParser ): \"\"\"HTML Soup Parser Attributes: - html (str): HTML content to parse - tag (str, None): HTML element tag returned by find - attrs (dict, None): HTML element attributes returned by find - text (str, None): HTML element text returned by find Methods: - find: return matching HTML elements {'auto', 'all', 'first'} Examples: ``` from gazpacho import Soup html = \"<div><p id='foo'>bar</p><p id='foo'>baz</p><p id='zoo'>bat</p></div>\" soup = Soup(html) soup.find('p', {'id': 'foo'}) # [<p id=\"foo\">bar</p>, <p id=\"foo\">baz</p>] result = soup.find('p', {'id': 'foo'}, mode='first') print(result) # <p id=\"foo\">bar</p> result = soup.find('p', {'id': 'zoo'}, mode='auto') print(result) # <p id=\"zoo\">bat</p> print(result.text) # bat ``` \"\"\" def __init__ ( self , html ): \"\"\"Params: - html (str): HTML content to parse \"\"\" super () . __init__ () self . html = html self . tag = None self . attrs = None self . text = None def __dir__ ( self ): return [ \"html\" , \"tag\" , \"attrs\" , \"text\" , \"find\" ] def __repr__ ( self ): return self . html @staticmethod def _empty_tag ( tag ): return tag in [ \"area\" , \"base\" , \"br\" , \"col\" , \"embed\" , \"hr\" , \"img\" , \"input\" , \"keygen\" , \"link\" , \"meta\" , \"param\" , \"source\" , \"track\" , \"wbr\" , ] def handle_starttag ( self , tag , attrs ): html , attrs = html_starttag_and_attrs ( tag , attrs ) matching = match ( self . attrs , attrs , self . strict ) if tag == self . tag and matching and not self . count : if not self . _empty_tag ( tag ): self . count += 1 self . group += 1 self . groups . append ( Soup ( \"\" )) self . groups [ self . group - 1 ] . html += html self . groups [ self . group - 1 ] . tag = tag self . groups [ self . group - 1 ] . attrs = attrs return if self . count : if not self . _empty_tag ( tag ): self . count += 1 self . groups [ self . group - 1 ] . html += html return def handle_startendtag ( self , tag , attrs ): html , attrs = html_starttag_and_attrs ( tag , attrs , True ) if self . count : self . groups [ self . group - 1 ] . html += html return def handle_data ( self , data ): if self . count : if self . groups [ self . group - 1 ] . text is None : self . groups [ self . group - 1 ] . text = data . strip () self . groups [ self . group - 1 ] . html += data return def handle_endtag ( self , tag ): if self . count : end_tag = f \"</{tag}>\" self . groups [ self . group - 1 ] . html += end_tag self . count -= 1 return def remove_tags ( self , strip = True ): \"\"\"Remove all HTML element tags Params: - strip (bool, True): Strip all extra whitespace Example: ``` html = '<span>Hi! I like <b>soup</b>.</span>' soup = Soup(html) soup.remove_tags() # Hi! I like soup. ``` \"\"\" text = re . sub ( \"<[^>]+>\" , \"\" , self . html ) if strip : text = \" \" . join ( text . split ()) return text def find ( self , tag , attrs = None , mode = \"auto\" , strict = False ): \"\"\"Return matching HTML elements Params: - tag (str): HTML element tag to find - attrs (dict, optional): HTML element attributes to match - mode (str, 'auto'): Element(s) to return {'auto', 'all', 'first'} - strict (bool, False): Require exact attribute matching Examples: ``` html = \"<div><p id='foo foo-striped'>bar</p><p id='foo'>baz</p><p id='zoo'>bat</p></div>\" soup = Soup(html) soup.find('p') # [<p id=\"foo foo-striped\">bar</p>, <p id=\"foo\">baz</p>, <p id=\"zoo\">bat</p>] soup.find('p', {'id': 'foo'}) # [<p id=\"foo foo-striped\">bar</p>, <p id=\"foo\">baz</p>] result = soup.find('p', {'id': 'foo'}, mode='first') print(result) # <p id=\"foo\">bar</p> soup.find('p', {'id': 'foo'}, strict=True) # [<p id=\"foo\">baz</p>] ``` \"\"\" self . tag = tag self . attrs = attrs self . strict = strict self . count = 0 self . group = 0 self . groups = [] self . feed ( self . html ) if mode in [ \"auto\" , \"first\" ] and not self . groups : return None if mode == \"all\" : return self . groups if mode == \"first\" : return self . groups [ 0 ] if mode == \"auto\" : if len ( self . groups ) == 1 : return self . groups [ 0 ] return self . groups Classes Soup class Soup ( html ) HTML Soup Parser Attributes: html (str): HTML content to parse tag (str, None): HTML element tag returned by find attrs (dict, None): HTML element attributes returned by find text (str, None): HTML element text returned by find Methods: find: return matching HTML elements {'auto', 'all', 'first'} Examples: from gazpacho import Soup html = \"<div><p id='foo'>bar</p><p id='foo'>baz</p><p id='zoo'>bat</p></div>\" soup = Soup ( html ) soup . find ( 'p' , { 'id' : 'foo' }) # [<p id=\"foo\">bar</p>, <p id=\"foo\">baz</p>] result = soup . find ( 'p' , { 'id' : 'foo' }, mode = 'first' ) print ( result ) # <p id=\"foo\">bar</p> result = soup . find ( 'p' , { 'id' : 'zoo' }, mode = 'auto' ) print ( result ) # <p id=\"zoo\">bat</p> print ( result . text ) # bat View Source class Soup ( HTMLParser ): \"\"\"HTML Soup Parser Attributes: - html (str): HTML content to parse - tag (str, None): HTML element tag returned by find - attrs (dict, None): HTML element attributes returned by find - text (str, None): HTML element text returned by find Methods: - find: return matching HTML elements {'auto', 'all', 'first'} Examples: ``` from gazpacho import Soup html = \"<div><p id='foo'>bar</p><p id='foo'>baz</p><p id='zoo'>bat</p></div>\" soup = Soup(html) soup.find('p', {'id': 'foo'}) # [<p id=\"foo\">bar</p>, <p id=\"foo\">baz</p>] result = soup.find('p', {'id': 'foo'}, mode='first') print(result) # <p id=\"foo\">bar</p> result = soup.find('p', {'id': 'zoo'}, mode='auto') print(result) # <p id=\"zoo\">bat</p> print(result.text) # bat ``` \"\"\" def __init__ ( self , html ): \"\"\"Params: - html (str): HTML content to parse \"\"\" super () . __init__ () self . html = html self . tag = None self . attrs = None self . text = None def __dir__ ( self ): return [ \"html\" , \"tag\" , \"attrs\" , \"text\" , \"find\" ] def __repr__ ( self ): return self . html @staticmethod def _empty_tag ( tag ): return tag in [ \"area\" , \"base\" , \"br\" , \"col\" , \"embed\" , \"hr\" , \"img\" , \"input\" , \"keygen\" , \"link\" , \"meta\" , \"param\" , \"source\" , \"track\" , \"wbr\" , ] def handle_starttag ( self , tag , attrs ): html , attrs = html_starttag_and_attrs ( tag , attrs ) matching = match ( self . attrs , attrs , self . strict ) if tag == self . tag and matching and not self . count : if not self . _empty_tag ( tag ): self . count += 1 self . group += 1 self . groups . append ( Soup ( \"\" )) self . groups [ self . group - 1 ] . html += html self . groups [ self . group - 1 ] . tag = tag self . groups [ self . group - 1 ] . attrs = attrs return if self . count : if not self . _empty_tag ( tag ): self . count += 1 self . groups [ self . group - 1 ] . html += html return def handle_startendtag ( self , tag , attrs ): html , attrs = html_starttag_and_attrs ( tag , attrs , True ) if self . count : self . groups [ self . group - 1 ] . html += html return def handle_data ( self , data ): if self . count : if self . groups [ self . group - 1 ] . text is None : self . groups [ self . group - 1 ] . text = data . strip () self . groups [ self . group - 1 ] . html += data return def handle_endtag ( self , tag ): if self . count : end_tag = f \"</{tag}>\" self . groups [ self . group - 1 ] . html += end_tag self . count -= 1 return def remove_tags ( self , strip = True ): \"\"\"Remove all HTML element tags Params: - strip (bool, True): Strip all extra whitespace Example: ``` html = '<span>Hi! I like <b>soup</b>.</span>' soup = Soup(html) soup.remove_tags() # Hi! I like soup. ``` \"\"\" text = re . sub ( \"<[^>]+>\" , \"\" , self . html ) if strip : text = \" \" . join ( text . split ()) return text def find ( self , tag , attrs = None , mode = \"auto\" , strict = False ): \"\"\"Return matching HTML elements Params: - tag (str): HTML element tag to find - attrs (dict, optional): HTML element attributes to match - mode (str, 'auto'): Element(s) to return {'auto', 'all', 'first'} - strict (bool, False): Require exact attribute matching Examples: ``` html = \"<div><p id='foo foo-striped'>bar</p><p id='foo'>baz</p><p id='zoo'>bat</p></div>\" soup = Soup(html) soup.find('p') # [<p id=\"foo foo-striped\">bar</p>, <p id=\"foo\">baz</p>, <p id=\"zoo\">bat</p>] soup.find('p', {'id': 'foo'}) # [<p id=\"foo foo-striped\">bar</p>, <p id=\"foo\">baz</p>] result = soup.find('p', {'id': 'foo'}, mode='first') print(result) # <p id=\"foo\">bar</p> soup.find('p', {'id': 'foo'}, strict=True) # [<p id=\"foo\">baz</p>] ``` \"\"\" self . tag = tag self . attrs = attrs self . strict = strict self . count = 0 self . group = 0 self . groups = [] self . feed ( self . html ) if mode in [ \"auto\" , \"first\" ] and not self . groups : return None if mode == \"all\" : return self . groups if mode == \"first\" : return self . groups [ 0 ] if mode == \"auto\" : if len ( self . groups ) == 1 : return self . groups [ 0 ] return self . groups Ancestors (in MRO) html.parser.HTMLParser _markupbase.ParserBase Class variables CDATA_CONTENT_ELEMENTS Methods check_for_whole_start_tag def check_for_whole_start_tag ( self , i ) View Source def check_for_whole_start_tag ( self , i ) : rawdata = self . rawdata m = locatestarttagend_tolerant . match ( rawdata , i ) if m : j = m . end () next = rawdata [ j : j + 1 ] if next == \" > \" : return j + 1 if next == \" / \" : if rawdata . startswith ( \" /> \" , j ) : return j + 2 if rawdata . startswith ( \" / \" , j ) : # buffer boundary return - 1 # else bogus input if j > i : return j else : return i + 1 if next == \"\" : # end of input return - 1 if next in ( \" abcdefghijklmnopqrstuvwxyz=/ \" \" ABCDEFGHIJKLMNOPQRSTUVWXYZ \" ) : # end of input in or before attribute value , or we have the # ' / ' from a ' /> ' ending return - 1 if j > i : return j else : return i + 1 raise AssertionError ( \" we should not get here! \" ) clear_cdata_mode def clear_cdata_mode ( self ) View Source def clear_cdata_mode ( self ): self . interesting = interesting_normal self . cdata_elem = None close def close ( self ) Handle any buffered data. View Source def close ( self ): \"\"\"Handle any buffered data.\"\"\" self . goahead ( 1 ) error def error ( self , message ) View Source def error ( self , message ): raise NotImplementedError ( \"subclasses of ParserBase must override error()\" ) feed def feed ( self , data ) Feed data to the parser. Call this as often as you want, with as little or as much text as you want (may include '\\n'). View Source def feed ( self , data ) : r \"\"\" Feed data to the parser. Call this as often as you want , with as little or as much text as you want ( may include ' \\n ' ) . \"\"\" self . rawdata = self . rawdata + data self . goahead ( 0 ) find def find ( self , tag , attrs = None , mode = 'auto' , strict = False ) Return matching HTML elements Params: tag (str): HTML element tag to find attrs (dict, optional): HTML element attributes to match mode (str, 'auto'): Element(s) to return {'auto', 'all', 'first'} strict (bool, False): Require exact attribute matching Examples: html = \" <div><p id= 'foo foo-striped' > bar </p><p id= 'foo' > baz </p><p id= 'zoo' > bat </p></div> \" soup = Soup(html) soup.find('p') # [ <p id= \"foo foo-striped\" > bar </p> , <p id= \"foo\" > baz </p> , <p id= \"zoo\" > bat </p> ] soup.find('p', {'id': 'foo'}) # [ <p id= \"foo foo-striped\" > bar </p> , <p id= \"foo\" > baz </p> ] result = soup.find('p', {'id': 'foo'}, mode='first') print(result) # <p id= \"foo\" > bar </p> soup.find('p', {'id': 'foo'}, strict=True) # [ <p id= \"foo\" > baz </p> ] View Source def find ( self , tag , attrs = None , mode = \" auto \" , strict = False ) : \"\"\" Return matching HTML elements Params : - tag ( str ) : HTML element tag to find - attrs ( dict , optional ) : HTML element attributes to match - mode ( str , ' auto ' ) : Element ( s ) to return { ' auto ' , ' all ' , ' first ' } - strict ( bool , False ) : Require exact attribute matching Examples : ``` html = \" <div><p id='foo foo-striped'>bar</p><p id='foo'>baz</p><p id='zoo'>bat</p></div> \" soup = Soup ( html ) soup . find ( ' p ' ) # [ < p id = \" foo foo-striped \" > bar </ p > , < p id = \" foo \" > baz </ p > , < p id = \" zoo \" > bat </ p > ] soup . find ( ' p ' , { ' id ' : ' foo ' } ) # [ < p id = \" foo foo-striped \" > bar </ p > , < p id = \" foo \" > baz </ p > ] result = soup . find ( ' p ' , { ' id ' : ' foo ' }, mode = ' first ' ) print ( result ) # < p id = \" foo \" > bar </ p > soup . find ( ' p ' , { ' id ' : ' foo ' }, strict = True ) # [ < p id = \" foo \" > baz </ p > ] ``` \"\"\" self . tag = tag self . attrs = attrs self . strict = strict self . count = 0 self . group = 0 self . groups = [] self . feed ( self . html ) if mode in [ \" auto \" , \" first \" ] and not self . groups : return None if mode == \" all \" : return self . groups if mode == \" first \" : return self . groups [ 0 ] if mode == \" auto \" : if len ( self . groups ) == 1 : return self . groups [ 0 ] return self . groups get_starttag_text def get_starttag_text ( self ) Return full source of start tag: '<...>'. View Source def get_starttag_text ( self ) : \"\"\" Return full source of start tag: '<...>'. \"\"\" return self . __starttag_text getpos def getpos ( self ) Return current line number and offset. View Source def getpos ( self ) : \"\"\" Return current line number and offset. \"\"\" return self . lineno , self . offset goahead def goahead ( self , end ) View Source def goahead ( self , end ) : rawdata = self . rawdata i = 0 n = len ( rawdata ) while i < n : if self . convert_charrefs and not self . cdata_elem: j = rawdata . find ( '<' , i ) if j < 0 : # if we can't find the next <, either we are at the end # or there's more text incoming . If the latter is True , # we can't pass the text to handle_data in case we have # a charref cut in half at end. Try to determine if # this is the case before proceeding by looking for an # & near the end and see if it's followed by a space or ;. amppos = rawdata . rfind ( '&' , max ( i , n - 34 )) if ( amppos >= 0 and not re . compile ( r'[\\s;]' ). search ( rawdata , amppos )) : break # wait till we get all the text j = n else : match = self . interesting . search ( rawdata , i ) # < or & if match : j = match . start () else : if self . cdata_elem: break j = n if i < j : if self . convert_charrefs and not self . cdata_elem: self . handle_data ( unescape ( rawdata [ i : j ])) else : self . handle_data ( rawdata [ i : j ]) i = self . updatepos ( i , j ) if i == n : break startswith = rawdata . startswith if startswith ( '<' , i ) : if starttagopen . match ( rawdata , i ) : # < + letter k = self . parse_starttag ( i ) elif startswith ( \"</\" , i ) : k = self . parse_endtag ( i ) elif startswith ( \"<!--\" , i ) : k = self . parse_comment ( i ) elif startswith ( \"<?\" , i ) : k = self . parse_pi ( i ) elif startswith ( \"<!\" , i ) : k = self . parse_html_declaration ( i ) elif ( i + 1 ) < n : self . handle_data ( \"<\" ) k = i + 1 else : break if k < 0 : if not end : break k = rawdata . find ( '>' , i + 1 ) if k < 0 : k = rawdata . find ( '<' , i + 1 ) if k < 0 : k = i + 1 else : k += 1 if self . convert_charrefs and not self . cdata_elem: self . handle_data ( unescape ( rawdata [ i : k ])) else : self . handle_data ( rawdata [ i : k ]) i = self . updatepos ( i , k ) elif startswith ( \"&#\" , i ) : match = charref . match ( rawdata , i ) if match : name = match . group ()[ 2 :- 1 ] self . handle_charref ( name ) k = match . end () if not startswith ( ';' , k - 1 ) : k = k - 1 i = self . updatepos ( i , k ) continue else : if \";\" in rawdata [ i :] : # bail by consuming &# self . handle_data ( rawdata [ i : i + 2 ]) i = self . updatepos ( i , i + 2 ) break elif startswith ( '&' , i ) : match = entityref . match ( rawdata , i ) if match : name = match . group ( 1 ) self . handle_entityref ( name ) k = match . end () if not startswith ( ';' , k - 1 ) : k = k - 1 i = self . updatepos ( i , k ) continue match = incomplete . match ( rawdata , i ) if match : # match . group () will contain at least 2 chars if end and match . group () == rawdata [ i :] : k = match . end () if k <= i : k = n i = self . updatepos ( i , i + 1 ) # incomplete break elif ( i + 1 ) < n : # not the end of the buffer , and can ' t be confused # with some other construct self . handle_data ( \"&\" ) i = self . updatepos ( i , i + 1 ) else : break else : assert 0 , \"interesting.search() lied\" # end while if end and i < n and not self . cdata_elem: if self . convert_charrefs and not self . cdata_elem: self . handle_data ( unescape ( rawdata [ i : n ])) else : self . handle_data ( rawdata [ i : n ]) i = self . updatepos ( i , n ) self . rawdata = rawdata [ i :] handle_charref def handle_charref ( self , name ) View Source def handle_charref ( self , name ): pass handle_comment def handle_comment ( self , data ) View Source def handle_comment ( self , data ): pass handle_data def handle_data ( self , data ) View Source def handle_data ( self , data ) : if self . count : if self . groups [ self . group - 1 ]. text is None : self . groups [ self . group - 1 ]. text = data . strip () self . groups [ self . group - 1 ]. html += data return handle_decl def handle_decl ( self , decl ) View Source def handle_decl ( self , decl ): pass handle_endtag def handle_endtag ( self , tag ) View Source def handle_endtag ( self , tag ) : if self . count : end_tag = f \" </{tag}> \" self . groups [ self . group - 1 ]. html += end_tag self . count -= 1 return handle_entityref def handle_entityref ( self , name ) View Source def handle_entityref ( self , name ): pass handle_pi def handle_pi ( self , data ) View Source def handle_pi ( self , data ): pass handle_startendtag def handle_startendtag ( self , tag , attrs ) View Source def handle_startendtag ( self , tag , attrs ) : html , attrs = html_starttag_and_attrs ( tag , attrs , True ) if self . count : self . groups [ self . group - 1 ]. html += html return handle_starttag def handle_starttag ( self , tag , attrs ) View Source def handle_starttag ( self , tag , attrs ) : html , attrs = html_starttag_and_attrs ( tag , attrs ) matching = match ( self . attrs , attrs , self . strict ) if tag == self . tag and matching and not self . count : if not self . _empty_tag ( tag ) : self . count += 1 self . group += 1 self . groups . append ( Soup ( \"\" )) self . groups [ self . group - 1 ]. html += html self . groups [ self . group - 1 ]. tag = tag self . groups [ self . group - 1 ]. attrs = attrs return if self . count : if not self . _empty_tag ( tag ) : self . count += 1 self . groups [ self . group - 1 ]. html += html return parse_bogus_comment def parse_bogus_comment ( self , i , report = 1 ) View Source def parse_bogus_comment ( self , i , report = 1 ) : rawdata = self . rawdata assert rawdata [ i : i + 2 ] in ( ' <! ' , ' </ ' ) , ( ' unexpected call to ' ' parse_comment() ' ) pos = rawdata . find ( ' > ' , i + 2 ) if pos == - 1 : return - 1 if report : self . handle_comment ( rawdata [ i + 2 : pos ] ) return pos + 1 parse_comment def parse_comment ( self , i , report = 1 ) View Source def parse_comment ( self , i , report = 1 ) : rawdata = self . rawdata if rawdata [ i : i + 4 ] != ' <!-- ' : self . error ( ' unexpected call to parse_comment() ' ) match = _commentclose . search ( rawdata , i + 4 ) if not match : return - 1 if report : j = match . start ( 0 ) self . handle_comment ( rawdata [ i + 4 : j ] ) return match . end ( 0 ) parse_declaration def parse_declaration ( self , i ) View Source def parse_declaration ( self , i ) : # This is some sort of declaration ; in \"HTML as # deployed,\" this should only be the document type # declaration (\"<!DOCTYPE html...>\"). # ISO 8879:1986, however, has more complex # declaration syntax for elements in <!...>, including: # --comment-- # [marked section] # name in the following list: ENTITY, DOCTYPE, ELEMENT, # ATTLIST, NOTATION, SHORTREF, USEMAP, # LINKTYPE, LINK, IDLINK, USELINK, SYSTEM rawdata = self . rawdata j = i + 2 assert rawdata [ i : j ] == \"<!\" , \"unexpected call to parse_declaration\" if rawdata [ j : j + 1 ] == \">\" : # the empty comment <!> return j + 1 if rawdata [ j : j + 1 ] in ( \"-\" , \"\" ) : # Start of comment followed by buffer boundary, # or just a buffer boundary. return - 1 # A simple, practical version could look like: ((name|stringlit) S*) + '>' n = len ( rawdata ) if rawdata [ j : j + 2 ] == ' -- ' : # comment # Locate --.*-- as the body of the comment return self . parse_comment ( i ) elif rawdata [ j ] == '[' : # marked section # Locate [statusWord [...arbitrary SGML...]] as the body of the marked section # Where statusWord is one of TEMP, CDATA, IGNORE, INCLUDE, RCDATA # Note that this is extended by Microsoft Office \"Save as Web\" function # to include [if...] and [endif]. return self . parse_marked_section ( i ) else : # all other declaration elements decltype , j = self . _scan_name ( j , i ) if j < 0 : return j if decltype == \"doctype\" : self . _decl_otherchars = '' while j < n : c = rawdata [ j ] if c == \">\" : # end of declaration syntax data = rawdata [ i + 2 : j ] if decltype == \"doctype\" : self . handle_decl ( data ) else : # According to the HTML5 specs sections \"8.2.4.44 Bogus # comment state\" and \"8.2.4.45 Markup declaration open # state\", a comment token should be emitted. # Calling unknown_decl provides more flexibility though. self . unknown_decl ( data ) return j + 1 if c in \" \\\" '\" : m = _declstringlit_match ( rawdata , j ) if not m : return - 1 # incomplete j = m . end () elif c in \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\" : name , j = self . _scan_name ( j , i ) elif c in self . _decl_otherchars : j = j + 1 elif c == \"[\" : # this could be handled in a separate doctype parser if decltype == \"doctype\" : j = self . _parse_doctype_subset ( j + 1 , i ) elif decltype in { \"attlist\" , \"linktype\" , \"link\" , \"element\" } : # must tolerate []'d groups in a content model in an element declaration # also in data attribute specifications of attlist declaration # also link type declaration subsets in linktype declarations # also link attribute specification lists in link declarations self . error ( \"unsupported '[' char in %s declaration\" % decltype ) else : self . error ( \"unexpected '[' char in declaration\" ) else : self . error ( \"unexpected %r char in declaration\" % rawdata [ j ]) if j < 0 : return j return - 1 # incomplete parse_endtag def parse_endtag ( self , i ) View Source def parse_endtag ( self , i ) : rawdata = self . rawdata assert rawdata [ i : i + 2 ] == \" </ \" , \" unexpected call to parse_endtag \" match = endendtag . search ( rawdata , i + 1 ) # > if not match : return - 1 gtpos = match . end () match = endtagfind . match ( rawdata , i ) # </ + tag + > if not match : if self . cdata_elem is not None : self . handle_data ( rawdata [ i : gtpos ] ) return gtpos # find the name : w3 . org / TR / html5 / tokenization . html # tag - name - state namematch = tagfind_tolerant . match ( rawdata , i + 2 ) if not namematch : # w3 . org / TR / html5 / tokenization . html # end - tag - open - state if rawdata [ i : i + 3 ] == ' </> ' : return i + 3 else : return self . parse_bogus_comment ( i ) tagname = namematch . group ( 1 ) . lower () # consume and ignore other stuff between the name and the > # Note : this is not 100 % correct , since we might have things like # </ tag attr = \" > \" > , but looking for > after tha name should cover # most of the cases and is much simpler gtpos = rawdata . find ( ' > ' , namematch . end ()) self . handle_endtag ( tagname ) return gtpos + 1 elem = match . group ( 1 ) . lower () # script or style if self . cdata_elem is not None : if elem != self . cdata_elem : self . handle_data ( rawdata [ i : gtpos ] ) return gtpos self . handle_endtag ( elem ) self . clear_cdata_mode () return gtpos parse_html_declaration def parse_html_declaration ( self , i ) View Source def parse_html_declaration ( self , i ) : rawdata = self . rawdata assert rawdata [ i : i + 2 ] == ' <! ' , ( ' unexpected call to ' ' parse_html_declaration() ' ) if rawdata [ i : i + 4 ] == ' <!-- ' : # this case is actually already handled in goahead () return self . parse_comment ( i ) elif rawdata [ i : i + 3 ] == ' <![ ' : return self . parse_marked_section ( i ) elif rawdata [ i : i + 9 ]. lower () == ' <!doctype ' : # find the closing > gtpos = rawdata . find ( ' > ' , i + 9 ) if gtpos == - 1 : return - 1 self . handle_decl ( rawdata [ i + 2 : gtpos ] ) return gtpos + 1 else : return self . parse_bogus_comment ( i ) parse_marked_section def parse_marked_section ( self , i , report = 1 ) View Source def parse_marked_section ( self , i , report = 1 ) : rawdata = self . rawdata assert rawdata [ i : i + 3 ] == ' <![ ' , \" unexpected call to parse_marked_section() \" sectName , j = self . _scan_name ( i + 3 , i ) if j < 0 : return j if sectName in { \" temp \" , \" cdata \" , \" ignore \" , \" include \" , \" rcdata \" }: # look for standard ]] > ending match = _markedsectionclose . search ( rawdata , i + 3 ) elif sectName in { \" if \" , \" else \" , \" endif \" }: # look for MS Office ] > ending match = _msmarkedsectionclose . search ( rawdata , i + 3 ) else : self . error ( ' unknown status keyword %r in marked section ' % rawdata [ i + 3 : j ] ) if not match : return - 1 if report : j = match . start ( 0 ) self . unknown_decl ( rawdata [ i + 3 : j ] ) return match . end ( 0 ) parse_pi def parse_pi ( self , i ) View Source def parse_pi ( self , i ) : rawdata = self . rawdata assert rawdata [ i : i + 2 ] == ' <? ' , ' unexpected call to parse_pi() ' match = piclose . search ( rawdata , i + 2 ) # > if not match : return - 1 j = match . start () self . handle_pi ( rawdata [ i + 2 : j ] ) j = match . end () return j parse_starttag def parse_starttag ( self , i ) View Source def parse_starttag ( self , i ) : self . __ starttag_text = None endpos = self . check_for_whole_start_tag ( i ) if endpos < 0 : return endpos rawdata = self . rawdata self . __ starttag_text = rawdata [ i : endpos ] # Now parse the data between i + 1 and j into a tag and attrs attrs = [] match = tagfind_tolerant . match ( rawdata , i + 1 ) assert match , 'unexpected call to parse_starttag()' k = match . end () self . lasttag = tag = match . group ( 1 ). lower () while k < endpos : m = attrfind_tolerant . match ( rawdata , k ) if not m : break attrname , rest , attrvalue = m . group ( 1 , 2 , 3 ) if not rest : attrvalue = None elif attrvalue [ : 1 ] == '\\'' == attrvalue[-1:] or \\ attrvalue[:1] == ' \"' == attrvalue[-1:]: attrvalue = attrvalue[1:-1] if attrvalue: attrvalue = unescape(attrvalue) attrs.append((attrname.lower(), attrvalue)) k = m.end() end = rawdata[k:endpos].strip() if end not in (\" > \", \" /> \"): lineno, offset = self.getpos() if \" \\n \" in self.__starttag_text: lineno = lineno + self.__starttag_text.count(\" \\n \") offset = len(self.__starttag_text) \\ - self.__starttag_text.rfind(\" \\n \") else: offset = offset + len(self.__starttag_text) self.handle_data(rawdata[i:endpos]) return endpos if end.endswith('/>'): # XHTML-style empty tag: <span attr=\" value \" /> self . handle_startendtag ( tag , attrs ) else : self . handle_starttag ( tag , attrs ) if tag in self . CDATA_CONTENT_ELEMENTS : self . set_cdata_mode ( tag ) return endpos remove_tags def remove_tags ( self , strip = True ) Remove all HTML element tags Params: strip (bool, True): Strip all extra whitespace Example: html = ' <span> Hi! I like <b> soup </b> . </span> ' soup = Soup(html) soup.remove_tags() # Hi! I like soup. View Source def remove_tags ( self , strip = True ) : \"\"\" Remove all HTML element tags Params : - strip ( bool , True ) : Strip all extra whitespace Example : ``` html = ' <span>Hi! I like <b>soup</b>.</span> ' soup = Soup ( html ) soup . remove_tags () # Hi ! I like soup . ``` \"\"\" text = re . sub ( \" <[^>]+> \" , \"\" , self . html ) if strip : text = \" \" . join ( text . split ()) return text reset def reset ( self ) Reset this instance. Loses all unprocessed data. View Source def reset ( self ): \"\"\"Reset this instance. Loses all unprocessed data.\"\"\" self . rawdata = '' self . lasttag = '???' self . interesting = interesting_normal self . cdata_elem = None _markupbase . ParserBase . reset ( self ) set_cdata_mode def set_cdata_mode ( self , elem ) View Source def set_cdata_mode ( self , elem ): self . cdata_elem = elem . lower () self . interesting = re . compile ( r '</\\s*%s\\s*>' % self . cdata_elem , re . I ) unescape def unescape ( self , s ) View Source def unescape ( self , s ) : warnings . warn ( ' The unescape method is deprecated and will be removed ' ' in 3.5, use html.unescape() instead. ' , DeprecationWarning , stacklevel = 2 ) return unescape ( s ) unknown_decl def unknown_decl ( self , data ) View Source def unknown_decl ( self , data ): pass updatepos def updatepos ( self , i , j ) View Source def updatepos ( self , i , j ) : if i >= j : return j rawdata = self . rawdata nlines = rawdata . count ( \" \\n \" , i , j ) if nlines : self . lineno = self . lineno + nlines pos = rawdata . rindex ( \" \\n \" , i , j ) # Should not fail self . offset = j - ( pos + 1 ) else : self . offset = self . offset + j - i return j","title":"Soup"},{"location":"reference/gazpacho/soup/#module-gazpachosoup","text":"View Source from html.parser import HTMLParser import re from .utils import match , html_starttag_and_attrs class Soup ( HTMLParser ): \"\"\"HTML Soup Parser Attributes: - html (str): HTML content to parse - tag (str, None): HTML element tag returned by find - attrs (dict, None): HTML element attributes returned by find - text (str, None): HTML element text returned by find Methods: - find: return matching HTML elements {'auto', 'all', 'first'} Examples: ``` from gazpacho import Soup html = \"<div><p id='foo'>bar</p><p id='foo'>baz</p><p id='zoo'>bat</p></div>\" soup = Soup(html) soup.find('p', {'id': 'foo'}) # [<p id=\"foo\">bar</p>, <p id=\"foo\">baz</p>] result = soup.find('p', {'id': 'foo'}, mode='first') print(result) # <p id=\"foo\">bar</p> result = soup.find('p', {'id': 'zoo'}, mode='auto') print(result) # <p id=\"zoo\">bat</p> print(result.text) # bat ``` \"\"\" def __init__ ( self , html ): \"\"\"Params: - html (str): HTML content to parse \"\"\" super () . __init__ () self . html = html self . tag = None self . attrs = None self . text = None def __dir__ ( self ): return [ \"html\" , \"tag\" , \"attrs\" , \"text\" , \"find\" ] def __repr__ ( self ): return self . html @staticmethod def _empty_tag ( tag ): return tag in [ \"area\" , \"base\" , \"br\" , \"col\" , \"embed\" , \"hr\" , \"img\" , \"input\" , \"keygen\" , \"link\" , \"meta\" , \"param\" , \"source\" , \"track\" , \"wbr\" , ] def handle_starttag ( self , tag , attrs ): html , attrs = html_starttag_and_attrs ( tag , attrs ) matching = match ( self . attrs , attrs , self . strict ) if tag == self . tag and matching and not self . count : if not self . _empty_tag ( tag ): self . count += 1 self . group += 1 self . groups . append ( Soup ( \"\" )) self . groups [ self . group - 1 ] . html += html self . groups [ self . group - 1 ] . tag = tag self . groups [ self . group - 1 ] . attrs = attrs return if self . count : if not self . _empty_tag ( tag ): self . count += 1 self . groups [ self . group - 1 ] . html += html return def handle_startendtag ( self , tag , attrs ): html , attrs = html_starttag_and_attrs ( tag , attrs , True ) if self . count : self . groups [ self . group - 1 ] . html += html return def handle_data ( self , data ): if self . count : if self . groups [ self . group - 1 ] . text is None : self . groups [ self . group - 1 ] . text = data . strip () self . groups [ self . group - 1 ] . html += data return def handle_endtag ( self , tag ): if self . count : end_tag = f \"</{tag}>\" self . groups [ self . group - 1 ] . html += end_tag self . count -= 1 return def remove_tags ( self , strip = True ): \"\"\"Remove all HTML element tags Params: - strip (bool, True): Strip all extra whitespace Example: ``` html = '<span>Hi! I like <b>soup</b>.</span>' soup = Soup(html) soup.remove_tags() # Hi! I like soup. ``` \"\"\" text = re . sub ( \"<[^>]+>\" , \"\" , self . html ) if strip : text = \" \" . join ( text . split ()) return text def find ( self , tag , attrs = None , mode = \"auto\" , strict = False ): \"\"\"Return matching HTML elements Params: - tag (str): HTML element tag to find - attrs (dict, optional): HTML element attributes to match - mode (str, 'auto'): Element(s) to return {'auto', 'all', 'first'} - strict (bool, False): Require exact attribute matching Examples: ``` html = \"<div><p id='foo foo-striped'>bar</p><p id='foo'>baz</p><p id='zoo'>bat</p></div>\" soup = Soup(html) soup.find('p') # [<p id=\"foo foo-striped\">bar</p>, <p id=\"foo\">baz</p>, <p id=\"zoo\">bat</p>] soup.find('p', {'id': 'foo'}) # [<p id=\"foo foo-striped\">bar</p>, <p id=\"foo\">baz</p>] result = soup.find('p', {'id': 'foo'}, mode='first') print(result) # <p id=\"foo\">bar</p> soup.find('p', {'id': 'foo'}, strict=True) # [<p id=\"foo\">baz</p>] ``` \"\"\" self . tag = tag self . attrs = attrs self . strict = strict self . count = 0 self . group = 0 self . groups = [] self . feed ( self . html ) if mode in [ \"auto\" , \"first\" ] and not self . groups : return None if mode == \"all\" : return self . groups if mode == \"first\" : return self . groups [ 0 ] if mode == \"auto\" : if len ( self . groups ) == 1 : return self . groups [ 0 ] return self . groups","title":"Module gazpacho.soup"},{"location":"reference/gazpacho/soup/#classes","text":"","title":"Classes"},{"location":"reference/gazpacho/soup/#soup","text":"class Soup ( html ) HTML Soup Parser Attributes: html (str): HTML content to parse tag (str, None): HTML element tag returned by find attrs (dict, None): HTML element attributes returned by find text (str, None): HTML element text returned by find Methods: find: return matching HTML elements {'auto', 'all', 'first'} Examples: from gazpacho import Soup html = \"<div><p id='foo'>bar</p><p id='foo'>baz</p><p id='zoo'>bat</p></div>\" soup = Soup ( html ) soup . find ( 'p' , { 'id' : 'foo' }) # [<p id=\"foo\">bar</p>, <p id=\"foo\">baz</p>] result = soup . find ( 'p' , { 'id' : 'foo' }, mode = 'first' ) print ( result ) # <p id=\"foo\">bar</p> result = soup . find ( 'p' , { 'id' : 'zoo' }, mode = 'auto' ) print ( result ) # <p id=\"zoo\">bat</p> print ( result . text ) # bat View Source class Soup ( HTMLParser ): \"\"\"HTML Soup Parser Attributes: - html (str): HTML content to parse - tag (str, None): HTML element tag returned by find - attrs (dict, None): HTML element attributes returned by find - text (str, None): HTML element text returned by find Methods: - find: return matching HTML elements {'auto', 'all', 'first'} Examples: ``` from gazpacho import Soup html = \"<div><p id='foo'>bar</p><p id='foo'>baz</p><p id='zoo'>bat</p></div>\" soup = Soup(html) soup.find('p', {'id': 'foo'}) # [<p id=\"foo\">bar</p>, <p id=\"foo\">baz</p>] result = soup.find('p', {'id': 'foo'}, mode='first') print(result) # <p id=\"foo\">bar</p> result = soup.find('p', {'id': 'zoo'}, mode='auto') print(result) # <p id=\"zoo\">bat</p> print(result.text) # bat ``` \"\"\" def __init__ ( self , html ): \"\"\"Params: - html (str): HTML content to parse \"\"\" super () . __init__ () self . html = html self . tag = None self . attrs = None self . text = None def __dir__ ( self ): return [ \"html\" , \"tag\" , \"attrs\" , \"text\" , \"find\" ] def __repr__ ( self ): return self . html @staticmethod def _empty_tag ( tag ): return tag in [ \"area\" , \"base\" , \"br\" , \"col\" , \"embed\" , \"hr\" , \"img\" , \"input\" , \"keygen\" , \"link\" , \"meta\" , \"param\" , \"source\" , \"track\" , \"wbr\" , ] def handle_starttag ( self , tag , attrs ): html , attrs = html_starttag_and_attrs ( tag , attrs ) matching = match ( self . attrs , attrs , self . strict ) if tag == self . tag and matching and not self . count : if not self . _empty_tag ( tag ): self . count += 1 self . group += 1 self . groups . append ( Soup ( \"\" )) self . groups [ self . group - 1 ] . html += html self . groups [ self . group - 1 ] . tag = tag self . groups [ self . group - 1 ] . attrs = attrs return if self . count : if not self . _empty_tag ( tag ): self . count += 1 self . groups [ self . group - 1 ] . html += html return def handle_startendtag ( self , tag , attrs ): html , attrs = html_starttag_and_attrs ( tag , attrs , True ) if self . count : self . groups [ self . group - 1 ] . html += html return def handle_data ( self , data ): if self . count : if self . groups [ self . group - 1 ] . text is None : self . groups [ self . group - 1 ] . text = data . strip () self . groups [ self . group - 1 ] . html += data return def handle_endtag ( self , tag ): if self . count : end_tag = f \"</{tag}>\" self . groups [ self . group - 1 ] . html += end_tag self . count -= 1 return def remove_tags ( self , strip = True ): \"\"\"Remove all HTML element tags Params: - strip (bool, True): Strip all extra whitespace Example: ``` html = '<span>Hi! I like <b>soup</b>.</span>' soup = Soup(html) soup.remove_tags() # Hi! I like soup. ``` \"\"\" text = re . sub ( \"<[^>]+>\" , \"\" , self . html ) if strip : text = \" \" . join ( text . split ()) return text def find ( self , tag , attrs = None , mode = \"auto\" , strict = False ): \"\"\"Return matching HTML elements Params: - tag (str): HTML element tag to find - attrs (dict, optional): HTML element attributes to match - mode (str, 'auto'): Element(s) to return {'auto', 'all', 'first'} - strict (bool, False): Require exact attribute matching Examples: ``` html = \"<div><p id='foo foo-striped'>bar</p><p id='foo'>baz</p><p id='zoo'>bat</p></div>\" soup = Soup(html) soup.find('p') # [<p id=\"foo foo-striped\">bar</p>, <p id=\"foo\">baz</p>, <p id=\"zoo\">bat</p>] soup.find('p', {'id': 'foo'}) # [<p id=\"foo foo-striped\">bar</p>, <p id=\"foo\">baz</p>] result = soup.find('p', {'id': 'foo'}, mode='first') print(result) # <p id=\"foo\">bar</p> soup.find('p', {'id': 'foo'}, strict=True) # [<p id=\"foo\">baz</p>] ``` \"\"\" self . tag = tag self . attrs = attrs self . strict = strict self . count = 0 self . group = 0 self . groups = [] self . feed ( self . html ) if mode in [ \"auto\" , \"first\" ] and not self . groups : return None if mode == \"all\" : return self . groups if mode == \"first\" : return self . groups [ 0 ] if mode == \"auto\" : if len ( self . groups ) == 1 : return self . groups [ 0 ] return self . groups","title":"Soup"},{"location":"reference/gazpacho/soup/#ancestors-in-mro","text":"html.parser.HTMLParser _markupbase.ParserBase","title":"Ancestors (in MRO)"},{"location":"reference/gazpacho/soup/#class-variables","text":"CDATA_CONTENT_ELEMENTS","title":"Class variables"},{"location":"reference/gazpacho/soup/#methods","text":"","title":"Methods"},{"location":"reference/gazpacho/soup/#check_for_whole_start_tag","text":"def check_for_whole_start_tag ( self , i ) View Source def check_for_whole_start_tag ( self , i ) : rawdata = self . rawdata m = locatestarttagend_tolerant . match ( rawdata , i ) if m : j = m . end () next = rawdata [ j : j + 1 ] if next == \" > \" : return j + 1 if next == \" / \" : if rawdata . startswith ( \" /> \" , j ) : return j + 2 if rawdata . startswith ( \" / \" , j ) : # buffer boundary return - 1 # else bogus input if j > i : return j else : return i + 1 if next == \"\" : # end of input return - 1 if next in ( \" abcdefghijklmnopqrstuvwxyz=/ \" \" ABCDEFGHIJKLMNOPQRSTUVWXYZ \" ) : # end of input in or before attribute value , or we have the # ' / ' from a ' /> ' ending return - 1 if j > i : return j else : return i + 1 raise AssertionError ( \" we should not get here! \" )","title":"check_for_whole_start_tag"},{"location":"reference/gazpacho/soup/#clear_cdata_mode","text":"def clear_cdata_mode ( self ) View Source def clear_cdata_mode ( self ): self . interesting = interesting_normal self . cdata_elem = None","title":"clear_cdata_mode"},{"location":"reference/gazpacho/soup/#close","text":"def close ( self ) Handle any buffered data. View Source def close ( self ): \"\"\"Handle any buffered data.\"\"\" self . goahead ( 1 )","title":"close"},{"location":"reference/gazpacho/soup/#error","text":"def error ( self , message ) View Source def error ( self , message ): raise NotImplementedError ( \"subclasses of ParserBase must override error()\" )","title":"error"},{"location":"reference/gazpacho/soup/#feed","text":"def feed ( self , data ) Feed data to the parser. Call this as often as you want, with as little or as much text as you want (may include '\\n'). View Source def feed ( self , data ) : r \"\"\" Feed data to the parser. Call this as often as you want , with as little or as much text as you want ( may include ' \\n ' ) . \"\"\" self . rawdata = self . rawdata + data self . goahead ( 0 )","title":"feed"},{"location":"reference/gazpacho/soup/#find","text":"def find ( self , tag , attrs = None , mode = 'auto' , strict = False ) Return matching HTML elements Params: tag (str): HTML element tag to find attrs (dict, optional): HTML element attributes to match mode (str, 'auto'): Element(s) to return {'auto', 'all', 'first'} strict (bool, False): Require exact attribute matching Examples: html = \" <div><p id= 'foo foo-striped' > bar </p><p id= 'foo' > baz </p><p id= 'zoo' > bat </p></div> \" soup = Soup(html) soup.find('p') # [ <p id= \"foo foo-striped\" > bar </p> , <p id= \"foo\" > baz </p> , <p id= \"zoo\" > bat </p> ] soup.find('p', {'id': 'foo'}) # [ <p id= \"foo foo-striped\" > bar </p> , <p id= \"foo\" > baz </p> ] result = soup.find('p', {'id': 'foo'}, mode='first') print(result) # <p id= \"foo\" > bar </p> soup.find('p', {'id': 'foo'}, strict=True) # [ <p id= \"foo\" > baz </p> ] View Source def find ( self , tag , attrs = None , mode = \" auto \" , strict = False ) : \"\"\" Return matching HTML elements Params : - tag ( str ) : HTML element tag to find - attrs ( dict , optional ) : HTML element attributes to match - mode ( str , ' auto ' ) : Element ( s ) to return { ' auto ' , ' all ' , ' first ' } - strict ( bool , False ) : Require exact attribute matching Examples : ``` html = \" <div><p id='foo foo-striped'>bar</p><p id='foo'>baz</p><p id='zoo'>bat</p></div> \" soup = Soup ( html ) soup . find ( ' p ' ) # [ < p id = \" foo foo-striped \" > bar </ p > , < p id = \" foo \" > baz </ p > , < p id = \" zoo \" > bat </ p > ] soup . find ( ' p ' , { ' id ' : ' foo ' } ) # [ < p id = \" foo foo-striped \" > bar </ p > , < p id = \" foo \" > baz </ p > ] result = soup . find ( ' p ' , { ' id ' : ' foo ' }, mode = ' first ' ) print ( result ) # < p id = \" foo \" > bar </ p > soup . find ( ' p ' , { ' id ' : ' foo ' }, strict = True ) # [ < p id = \" foo \" > baz </ p > ] ``` \"\"\" self . tag = tag self . attrs = attrs self . strict = strict self . count = 0 self . group = 0 self . groups = [] self . feed ( self . html ) if mode in [ \" auto \" , \" first \" ] and not self . groups : return None if mode == \" all \" : return self . groups if mode == \" first \" : return self . groups [ 0 ] if mode == \" auto \" : if len ( self . groups ) == 1 : return self . groups [ 0 ] return self . groups","title":"find"},{"location":"reference/gazpacho/soup/#get_starttag_text","text":"def get_starttag_text ( self ) Return full source of start tag: '<...>'. View Source def get_starttag_text ( self ) : \"\"\" Return full source of start tag: '<...>'. \"\"\" return self . __starttag_text","title":"get_starttag_text"},{"location":"reference/gazpacho/soup/#getpos","text":"def getpos ( self ) Return current line number and offset. View Source def getpos ( self ) : \"\"\" Return current line number and offset. \"\"\" return self . lineno , self . offset","title":"getpos"},{"location":"reference/gazpacho/soup/#goahead","text":"def goahead ( self , end ) View Source def goahead ( self , end ) : rawdata = self . rawdata i = 0 n = len ( rawdata ) while i < n : if self . convert_charrefs and not self . cdata_elem: j = rawdata . find ( '<' , i ) if j < 0 : # if we can't find the next <, either we are at the end # or there's more text incoming . If the latter is True , # we can't pass the text to handle_data in case we have # a charref cut in half at end. Try to determine if # this is the case before proceeding by looking for an # & near the end and see if it's followed by a space or ;. amppos = rawdata . rfind ( '&' , max ( i , n - 34 )) if ( amppos >= 0 and not re . compile ( r'[\\s;]' ). search ( rawdata , amppos )) : break # wait till we get all the text j = n else : match = self . interesting . search ( rawdata , i ) # < or & if match : j = match . start () else : if self . cdata_elem: break j = n if i < j : if self . convert_charrefs and not self . cdata_elem: self . handle_data ( unescape ( rawdata [ i : j ])) else : self . handle_data ( rawdata [ i : j ]) i = self . updatepos ( i , j ) if i == n : break startswith = rawdata . startswith if startswith ( '<' , i ) : if starttagopen . match ( rawdata , i ) : # < + letter k = self . parse_starttag ( i ) elif startswith ( \"</\" , i ) : k = self . parse_endtag ( i ) elif startswith ( \"<!--\" , i ) : k = self . parse_comment ( i ) elif startswith ( \"<?\" , i ) : k = self . parse_pi ( i ) elif startswith ( \"<!\" , i ) : k = self . parse_html_declaration ( i ) elif ( i + 1 ) < n : self . handle_data ( \"<\" ) k = i + 1 else : break if k < 0 : if not end : break k = rawdata . find ( '>' , i + 1 ) if k < 0 : k = rawdata . find ( '<' , i + 1 ) if k < 0 : k = i + 1 else : k += 1 if self . convert_charrefs and not self . cdata_elem: self . handle_data ( unescape ( rawdata [ i : k ])) else : self . handle_data ( rawdata [ i : k ]) i = self . updatepos ( i , k ) elif startswith ( \"&#\" , i ) : match = charref . match ( rawdata , i ) if match : name = match . group ()[ 2 :- 1 ] self . handle_charref ( name ) k = match . end () if not startswith ( ';' , k - 1 ) : k = k - 1 i = self . updatepos ( i , k ) continue else : if \";\" in rawdata [ i :] : # bail by consuming &# self . handle_data ( rawdata [ i : i + 2 ]) i = self . updatepos ( i , i + 2 ) break elif startswith ( '&' , i ) : match = entityref . match ( rawdata , i ) if match : name = match . group ( 1 ) self . handle_entityref ( name ) k = match . end () if not startswith ( ';' , k - 1 ) : k = k - 1 i = self . updatepos ( i , k ) continue match = incomplete . match ( rawdata , i ) if match : # match . group () will contain at least 2 chars if end and match . group () == rawdata [ i :] : k = match . end () if k <= i : k = n i = self . updatepos ( i , i + 1 ) # incomplete break elif ( i + 1 ) < n : # not the end of the buffer , and can ' t be confused # with some other construct self . handle_data ( \"&\" ) i = self . updatepos ( i , i + 1 ) else : break else : assert 0 , \"interesting.search() lied\" # end while if end and i < n and not self . cdata_elem: if self . convert_charrefs and not self . cdata_elem: self . handle_data ( unescape ( rawdata [ i : n ])) else : self . handle_data ( rawdata [ i : n ]) i = self . updatepos ( i , n ) self . rawdata = rawdata [ i :]","title":"goahead"},{"location":"reference/gazpacho/soup/#handle_charref","text":"def handle_charref ( self , name ) View Source def handle_charref ( self , name ): pass","title":"handle_charref"},{"location":"reference/gazpacho/soup/#handle_comment","text":"def handle_comment ( self , data ) View Source def handle_comment ( self , data ): pass","title":"handle_comment"},{"location":"reference/gazpacho/soup/#handle_data","text":"def handle_data ( self , data ) View Source def handle_data ( self , data ) : if self . count : if self . groups [ self . group - 1 ]. text is None : self . groups [ self . group - 1 ]. text = data . strip () self . groups [ self . group - 1 ]. html += data return","title":"handle_data"},{"location":"reference/gazpacho/soup/#handle_decl","text":"def handle_decl ( self , decl ) View Source def handle_decl ( self , decl ): pass","title":"handle_decl"},{"location":"reference/gazpacho/soup/#handle_endtag","text":"def handle_endtag ( self , tag ) View Source def handle_endtag ( self , tag ) : if self . count : end_tag = f \" </{tag}> \" self . groups [ self . group - 1 ]. html += end_tag self . count -= 1 return","title":"handle_endtag"},{"location":"reference/gazpacho/soup/#handle_entityref","text":"def handle_entityref ( self , name ) View Source def handle_entityref ( self , name ): pass","title":"handle_entityref"},{"location":"reference/gazpacho/soup/#handle_pi","text":"def handle_pi ( self , data ) View Source def handle_pi ( self , data ): pass","title":"handle_pi"},{"location":"reference/gazpacho/soup/#handle_startendtag","text":"def handle_startendtag ( self , tag , attrs ) View Source def handle_startendtag ( self , tag , attrs ) : html , attrs = html_starttag_and_attrs ( tag , attrs , True ) if self . count : self . groups [ self . group - 1 ]. html += html return","title":"handle_startendtag"},{"location":"reference/gazpacho/soup/#handle_starttag","text":"def handle_starttag ( self , tag , attrs ) View Source def handle_starttag ( self , tag , attrs ) : html , attrs = html_starttag_and_attrs ( tag , attrs ) matching = match ( self . attrs , attrs , self . strict ) if tag == self . tag and matching and not self . count : if not self . _empty_tag ( tag ) : self . count += 1 self . group += 1 self . groups . append ( Soup ( \"\" )) self . groups [ self . group - 1 ]. html += html self . groups [ self . group - 1 ]. tag = tag self . groups [ self . group - 1 ]. attrs = attrs return if self . count : if not self . _empty_tag ( tag ) : self . count += 1 self . groups [ self . group - 1 ]. html += html return","title":"handle_starttag"},{"location":"reference/gazpacho/soup/#parse_bogus_comment","text":"def parse_bogus_comment ( self , i , report = 1 ) View Source def parse_bogus_comment ( self , i , report = 1 ) : rawdata = self . rawdata assert rawdata [ i : i + 2 ] in ( ' <! ' , ' </ ' ) , ( ' unexpected call to ' ' parse_comment() ' ) pos = rawdata . find ( ' > ' , i + 2 ) if pos == - 1 : return - 1 if report : self . handle_comment ( rawdata [ i + 2 : pos ] ) return pos + 1","title":"parse_bogus_comment"},{"location":"reference/gazpacho/soup/#parse_comment","text":"def parse_comment ( self , i , report = 1 ) View Source def parse_comment ( self , i , report = 1 ) : rawdata = self . rawdata if rawdata [ i : i + 4 ] != ' <!-- ' : self . error ( ' unexpected call to parse_comment() ' ) match = _commentclose . search ( rawdata , i + 4 ) if not match : return - 1 if report : j = match . start ( 0 ) self . handle_comment ( rawdata [ i + 4 : j ] ) return match . end ( 0 )","title":"parse_comment"},{"location":"reference/gazpacho/soup/#parse_declaration","text":"def parse_declaration ( self , i ) View Source def parse_declaration ( self , i ) : # This is some sort of declaration ; in \"HTML as # deployed,\" this should only be the document type # declaration (\"<!DOCTYPE html...>\"). # ISO 8879:1986, however, has more complex # declaration syntax for elements in <!...>, including: # --comment-- # [marked section] # name in the following list: ENTITY, DOCTYPE, ELEMENT, # ATTLIST, NOTATION, SHORTREF, USEMAP, # LINKTYPE, LINK, IDLINK, USELINK, SYSTEM rawdata = self . rawdata j = i + 2 assert rawdata [ i : j ] == \"<!\" , \"unexpected call to parse_declaration\" if rawdata [ j : j + 1 ] == \">\" : # the empty comment <!> return j + 1 if rawdata [ j : j + 1 ] in ( \"-\" , \"\" ) : # Start of comment followed by buffer boundary, # or just a buffer boundary. return - 1 # A simple, practical version could look like: ((name|stringlit) S*) + '>' n = len ( rawdata ) if rawdata [ j : j + 2 ] == ' -- ' : # comment # Locate --.*-- as the body of the comment return self . parse_comment ( i ) elif rawdata [ j ] == '[' : # marked section # Locate [statusWord [...arbitrary SGML...]] as the body of the marked section # Where statusWord is one of TEMP, CDATA, IGNORE, INCLUDE, RCDATA # Note that this is extended by Microsoft Office \"Save as Web\" function # to include [if...] and [endif]. return self . parse_marked_section ( i ) else : # all other declaration elements decltype , j = self . _scan_name ( j , i ) if j < 0 : return j if decltype == \"doctype\" : self . _decl_otherchars = '' while j < n : c = rawdata [ j ] if c == \">\" : # end of declaration syntax data = rawdata [ i + 2 : j ] if decltype == \"doctype\" : self . handle_decl ( data ) else : # According to the HTML5 specs sections \"8.2.4.44 Bogus # comment state\" and \"8.2.4.45 Markup declaration open # state\", a comment token should be emitted. # Calling unknown_decl provides more flexibility though. self . unknown_decl ( data ) return j + 1 if c in \" \\\" '\" : m = _declstringlit_match ( rawdata , j ) if not m : return - 1 # incomplete j = m . end () elif c in \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\" : name , j = self . _scan_name ( j , i ) elif c in self . _decl_otherchars : j = j + 1 elif c == \"[\" : # this could be handled in a separate doctype parser if decltype == \"doctype\" : j = self . _parse_doctype_subset ( j + 1 , i ) elif decltype in { \"attlist\" , \"linktype\" , \"link\" , \"element\" } : # must tolerate []'d groups in a content model in an element declaration # also in data attribute specifications of attlist declaration # also link type declaration subsets in linktype declarations # also link attribute specification lists in link declarations self . error ( \"unsupported '[' char in %s declaration\" % decltype ) else : self . error ( \"unexpected '[' char in declaration\" ) else : self . error ( \"unexpected %r char in declaration\" % rawdata [ j ]) if j < 0 : return j return - 1 # incomplete","title":"parse_declaration"},{"location":"reference/gazpacho/soup/#parse_endtag","text":"def parse_endtag ( self , i ) View Source def parse_endtag ( self , i ) : rawdata = self . rawdata assert rawdata [ i : i + 2 ] == \" </ \" , \" unexpected call to parse_endtag \" match = endendtag . search ( rawdata , i + 1 ) # > if not match : return - 1 gtpos = match . end () match = endtagfind . match ( rawdata , i ) # </ + tag + > if not match : if self . cdata_elem is not None : self . handle_data ( rawdata [ i : gtpos ] ) return gtpos # find the name : w3 . org / TR / html5 / tokenization . html # tag - name - state namematch = tagfind_tolerant . match ( rawdata , i + 2 ) if not namematch : # w3 . org / TR / html5 / tokenization . html # end - tag - open - state if rawdata [ i : i + 3 ] == ' </> ' : return i + 3 else : return self . parse_bogus_comment ( i ) tagname = namematch . group ( 1 ) . lower () # consume and ignore other stuff between the name and the > # Note : this is not 100 % correct , since we might have things like # </ tag attr = \" > \" > , but looking for > after tha name should cover # most of the cases and is much simpler gtpos = rawdata . find ( ' > ' , namematch . end ()) self . handle_endtag ( tagname ) return gtpos + 1 elem = match . group ( 1 ) . lower () # script or style if self . cdata_elem is not None : if elem != self . cdata_elem : self . handle_data ( rawdata [ i : gtpos ] ) return gtpos self . handle_endtag ( elem ) self . clear_cdata_mode () return gtpos","title":"parse_endtag"},{"location":"reference/gazpacho/soup/#parse_html_declaration","text":"def parse_html_declaration ( self , i ) View Source def parse_html_declaration ( self , i ) : rawdata = self . rawdata assert rawdata [ i : i + 2 ] == ' <! ' , ( ' unexpected call to ' ' parse_html_declaration() ' ) if rawdata [ i : i + 4 ] == ' <!-- ' : # this case is actually already handled in goahead () return self . parse_comment ( i ) elif rawdata [ i : i + 3 ] == ' <![ ' : return self . parse_marked_section ( i ) elif rawdata [ i : i + 9 ]. lower () == ' <!doctype ' : # find the closing > gtpos = rawdata . find ( ' > ' , i + 9 ) if gtpos == - 1 : return - 1 self . handle_decl ( rawdata [ i + 2 : gtpos ] ) return gtpos + 1 else : return self . parse_bogus_comment ( i )","title":"parse_html_declaration"},{"location":"reference/gazpacho/soup/#parse_marked_section","text":"def parse_marked_section ( self , i , report = 1 ) View Source def parse_marked_section ( self , i , report = 1 ) : rawdata = self . rawdata assert rawdata [ i : i + 3 ] == ' <![ ' , \" unexpected call to parse_marked_section() \" sectName , j = self . _scan_name ( i + 3 , i ) if j < 0 : return j if sectName in { \" temp \" , \" cdata \" , \" ignore \" , \" include \" , \" rcdata \" }: # look for standard ]] > ending match = _markedsectionclose . search ( rawdata , i + 3 ) elif sectName in { \" if \" , \" else \" , \" endif \" }: # look for MS Office ] > ending match = _msmarkedsectionclose . search ( rawdata , i + 3 ) else : self . error ( ' unknown status keyword %r in marked section ' % rawdata [ i + 3 : j ] ) if not match : return - 1 if report : j = match . start ( 0 ) self . unknown_decl ( rawdata [ i + 3 : j ] ) return match . end ( 0 )","title":"parse_marked_section"},{"location":"reference/gazpacho/soup/#parse_pi","text":"def parse_pi ( self , i ) View Source def parse_pi ( self , i ) : rawdata = self . rawdata assert rawdata [ i : i + 2 ] == ' <? ' , ' unexpected call to parse_pi() ' match = piclose . search ( rawdata , i + 2 ) # > if not match : return - 1 j = match . start () self . handle_pi ( rawdata [ i + 2 : j ] ) j = match . end () return j","title":"parse_pi"},{"location":"reference/gazpacho/soup/#parse_starttag","text":"def parse_starttag ( self , i ) View Source def parse_starttag ( self , i ) : self . __ starttag_text = None endpos = self . check_for_whole_start_tag ( i ) if endpos < 0 : return endpos rawdata = self . rawdata self . __ starttag_text = rawdata [ i : endpos ] # Now parse the data between i + 1 and j into a tag and attrs attrs = [] match = tagfind_tolerant . match ( rawdata , i + 1 ) assert match , 'unexpected call to parse_starttag()' k = match . end () self . lasttag = tag = match . group ( 1 ). lower () while k < endpos : m = attrfind_tolerant . match ( rawdata , k ) if not m : break attrname , rest , attrvalue = m . group ( 1 , 2 , 3 ) if not rest : attrvalue = None elif attrvalue [ : 1 ] == '\\'' == attrvalue[-1:] or \\ attrvalue[:1] == ' \"' == attrvalue[-1:]: attrvalue = attrvalue[1:-1] if attrvalue: attrvalue = unescape(attrvalue) attrs.append((attrname.lower(), attrvalue)) k = m.end() end = rawdata[k:endpos].strip() if end not in (\" > \", \" /> \"): lineno, offset = self.getpos() if \" \\n \" in self.__starttag_text: lineno = lineno + self.__starttag_text.count(\" \\n \") offset = len(self.__starttag_text) \\ - self.__starttag_text.rfind(\" \\n \") else: offset = offset + len(self.__starttag_text) self.handle_data(rawdata[i:endpos]) return endpos if end.endswith('/>'): # XHTML-style empty tag: <span attr=\" value \" /> self . handle_startendtag ( tag , attrs ) else : self . handle_starttag ( tag , attrs ) if tag in self . CDATA_CONTENT_ELEMENTS : self . set_cdata_mode ( tag ) return endpos","title":"parse_starttag"},{"location":"reference/gazpacho/soup/#remove_tags","text":"def remove_tags ( self , strip = True ) Remove all HTML element tags Params: strip (bool, True): Strip all extra whitespace Example: html = ' <span> Hi! I like <b> soup </b> . </span> ' soup = Soup(html) soup.remove_tags() # Hi! I like soup. View Source def remove_tags ( self , strip = True ) : \"\"\" Remove all HTML element tags Params : - strip ( bool , True ) : Strip all extra whitespace Example : ``` html = ' <span>Hi! I like <b>soup</b>.</span> ' soup = Soup ( html ) soup . remove_tags () # Hi ! I like soup . ``` \"\"\" text = re . sub ( \" <[^>]+> \" , \"\" , self . html ) if strip : text = \" \" . join ( text . split ()) return text","title":"remove_tags"},{"location":"reference/gazpacho/soup/#reset","text":"def reset ( self ) Reset this instance. Loses all unprocessed data. View Source def reset ( self ): \"\"\"Reset this instance. Loses all unprocessed data.\"\"\" self . rawdata = '' self . lasttag = '???' self . interesting = interesting_normal self . cdata_elem = None _markupbase . ParserBase . reset ( self )","title":"reset"},{"location":"reference/gazpacho/soup/#set_cdata_mode","text":"def set_cdata_mode ( self , elem ) View Source def set_cdata_mode ( self , elem ): self . cdata_elem = elem . lower () self . interesting = re . compile ( r '</\\s*%s\\s*>' % self . cdata_elem , re . I )","title":"set_cdata_mode"},{"location":"reference/gazpacho/soup/#unescape","text":"def unescape ( self , s ) View Source def unescape ( self , s ) : warnings . warn ( ' The unescape method is deprecated and will be removed ' ' in 3.5, use html.unescape() instead. ' , DeprecationWarning , stacklevel = 2 ) return unescape ( s )","title":"unescape"},{"location":"reference/gazpacho/soup/#unknown_decl","text":"def unknown_decl ( self , data ) View Source def unknown_decl ( self , data ): pass","title":"unknown_decl"},{"location":"reference/gazpacho/soup/#updatepos","text":"def updatepos ( self , i , j ) View Source def updatepos ( self , i , j ) : if i >= j : return j rawdata = self . rawdata nlines = rawdata . count ( \" \\n \" , i , j ) if nlines : self . lineno = self . lineno + nlines pos = rawdata . rindex ( \" \\n \" , i , j ) # Should not fail self . offset = j - ( pos + 1 ) else : self . offset = self . offset + j - i return j","title":"updatepos"},{"location":"reference/gazpacho/utils/","text":"Module gazpacho.utils View Source def match ( a , b , strict = False ) : \"\"\" Utility function to match two dictionaries Params : - a ( dict ) : Query dictionary - b ( dict ) : Dictionary to match - strict ( bool ) : Require exact matching Examples : ``` a = { ' foo ' : ' bar ' } b = { ' foo ' : ' bar baz ' } match ( a , b ) # True a = { ' foo ' : ' bar ' } b = { ' foo ' : ' bar baz ' } match ( a , b , strict = True ) # False a = {} b = { ' foo ' : ' bar ' } match ( a , b ) # True a = {} b = {} match ( a , b ) # True ``` \"\"\" if not a : return True if not a and not b : return True if a and not b : return False for k , v in a . items () : if not b . get ( k ) : return False if strict : if v == b . get ( k ) : continue else : return False if v in b . get ( k ) : continue else : return False return True def html_starttag_and_attrs ( tag , attrs , startendtag = False ) : \"\"\" Utility functon to reconstruct starttag and attrs Params : - tag ( str ) : HTML element tag - attrs ( list ) : HTML element attributes formatted as a list of tuples - startendtag ( bool , False ) : Flag to handle startend tags Example : ``` html_starttag_and_attrs ( ' a ' , [ ( ' href ' , ' localhost:8000 ' ) ] ) # ( ' <a href=\"localhost:8000\"> ' , { ' href ' : ' localhost:8000 ' } ) ``` \"\"\" if attrs : attrs = dict ( attrs ) af = [ f ' {k}=\"{v}\" ' for k , v in attrs . items () ] af = f ' {\" \".join(af)} ' else : attrs = {} af = \"\" if startendtag : html = f \" <{tag}{af} /> \" else : html = f \" <{tag}{af}> \" return html , attrs Functions html_starttag_and_attrs def html_starttag_and_attrs ( tag , attrs , startendtag = False ) Utility functon to reconstruct starttag and attrs Params: tag (str): HTML element tag attrs (list): HTML element attributes formatted as a list of tuples startendtag (bool, False): Flag to handle startend tags Example: html_starttag_and_attrs ( 'a' , [( 'href' , 'localhost:8000' )]) # ( '<a href=\"localhost:8000\">' , { 'href' : 'localhost:8000' } ) View Source def html_starttag_and_attrs ( tag , attrs , startendtag = False ) : \"\"\" Utility functon to reconstruct starttag and attrs Params : - tag ( str ) : HTML element tag - attrs ( list ) : HTML element attributes formatted as a list of tuples - startendtag ( bool , False ) : Flag to handle startend tags Example : ``` html_starttag_and_attrs ( ' a ' , [ ( ' href ' , ' localhost:8000 ' ) ] ) # ( ' <a href=\"localhost:8000\"> ' , { ' href ' : ' localhost:8000 ' } ) ``` \"\"\" if attrs : attrs = dict ( attrs ) af = [ f ' {k}=\"{v}\" ' for k , v in attrs . items () ] af = f ' {\" \".join(af)} ' else : attrs = {} af = \"\" if startendtag : html = f \" <{tag}{af} /> \" else : html = f \" <{tag}{af}> \" return html , attrs match def match ( a , b , strict = False ) Utility function to match two dictionaries Params: a (dict): Query dictionary b (dict): Dictionary to match strict (bool): Require exact matching Examples: a = { 'foo' : 'bar' } b = { 'foo' : 'bar baz' } match ( a , b ) # True a = { 'foo' : 'bar' } b = { 'foo' : 'bar baz' } match ( a , b , strict = True ) # False a = {} b = { 'foo' : 'bar' } match ( a , b ) # True a = {} b = {} match ( a , b ) # True View Source def match ( a , b , strict = False ) : \"\"\" Utility function to match two dictionaries Params : - a ( dict ) : Query dictionary - b ( dict ) : Dictionary to match - strict ( bool ) : Require exact matching Examples : ``` a = { ' foo ' : ' bar ' } b = { ' foo ' : ' bar baz ' } match ( a , b ) # True a = { ' foo ' : ' bar ' } b = { ' foo ' : ' bar baz ' } match ( a , b , strict = True ) # False a = {} b = { ' foo ' : ' bar ' } match ( a , b ) # True a = {} b = {} match ( a , b ) # True ``` \"\"\" if not a : return True if not a and not b : return True if a and not b : return False for k , v in a . items () : if not b . get ( k ) : return False if strict : if v == b . get ( k ) : continue else : return False if v in b . get ( k ) : continue else : return False return True","title":"Utils"},{"location":"reference/gazpacho/utils/#module-gazpachoutils","text":"View Source def match ( a , b , strict = False ) : \"\"\" Utility function to match two dictionaries Params : - a ( dict ) : Query dictionary - b ( dict ) : Dictionary to match - strict ( bool ) : Require exact matching Examples : ``` a = { ' foo ' : ' bar ' } b = { ' foo ' : ' bar baz ' } match ( a , b ) # True a = { ' foo ' : ' bar ' } b = { ' foo ' : ' bar baz ' } match ( a , b , strict = True ) # False a = {} b = { ' foo ' : ' bar ' } match ( a , b ) # True a = {} b = {} match ( a , b ) # True ``` \"\"\" if not a : return True if not a and not b : return True if a and not b : return False for k , v in a . items () : if not b . get ( k ) : return False if strict : if v == b . get ( k ) : continue else : return False if v in b . get ( k ) : continue else : return False return True def html_starttag_and_attrs ( tag , attrs , startendtag = False ) : \"\"\" Utility functon to reconstruct starttag and attrs Params : - tag ( str ) : HTML element tag - attrs ( list ) : HTML element attributes formatted as a list of tuples - startendtag ( bool , False ) : Flag to handle startend tags Example : ``` html_starttag_and_attrs ( ' a ' , [ ( ' href ' , ' localhost:8000 ' ) ] ) # ( ' <a href=\"localhost:8000\"> ' , { ' href ' : ' localhost:8000 ' } ) ``` \"\"\" if attrs : attrs = dict ( attrs ) af = [ f ' {k}=\"{v}\" ' for k , v in attrs . items () ] af = f ' {\" \".join(af)} ' else : attrs = {} af = \"\" if startendtag : html = f \" <{tag}{af} /> \" else : html = f \" <{tag}{af}> \" return html , attrs","title":"Module gazpacho.utils"},{"location":"reference/gazpacho/utils/#functions","text":"","title":"Functions"},{"location":"reference/gazpacho/utils/#html_starttag_and_attrs","text":"def html_starttag_and_attrs ( tag , attrs , startendtag = False ) Utility functon to reconstruct starttag and attrs Params: tag (str): HTML element tag attrs (list): HTML element attributes formatted as a list of tuples startendtag (bool, False): Flag to handle startend tags Example: html_starttag_and_attrs ( 'a' , [( 'href' , 'localhost:8000' )]) # ( '<a href=\"localhost:8000\">' , { 'href' : 'localhost:8000' } ) View Source def html_starttag_and_attrs ( tag , attrs , startendtag = False ) : \"\"\" Utility functon to reconstruct starttag and attrs Params : - tag ( str ) : HTML element tag - attrs ( list ) : HTML element attributes formatted as a list of tuples - startendtag ( bool , False ) : Flag to handle startend tags Example : ``` html_starttag_and_attrs ( ' a ' , [ ( ' href ' , ' localhost:8000 ' ) ] ) # ( ' <a href=\"localhost:8000\"> ' , { ' href ' : ' localhost:8000 ' } ) ``` \"\"\" if attrs : attrs = dict ( attrs ) af = [ f ' {k}=\"{v}\" ' for k , v in attrs . items () ] af = f ' {\" \".join(af)} ' else : attrs = {} af = \"\" if startendtag : html = f \" <{tag}{af} /> \" else : html = f \" <{tag}{af}> \" return html , attrs","title":"html_starttag_and_attrs"},{"location":"reference/gazpacho/utils/#match","text":"def match ( a , b , strict = False ) Utility function to match two dictionaries Params: a (dict): Query dictionary b (dict): Dictionary to match strict (bool): Require exact matching Examples: a = { 'foo' : 'bar' } b = { 'foo' : 'bar baz' } match ( a , b ) # True a = { 'foo' : 'bar' } b = { 'foo' : 'bar baz' } match ( a , b , strict = True ) # False a = {} b = { 'foo' : 'bar' } match ( a , b ) # True a = {} b = {} match ( a , b ) # True View Source def match ( a , b , strict = False ) : \"\"\" Utility function to match two dictionaries Params : - a ( dict ) : Query dictionary - b ( dict ) : Dictionary to match - strict ( bool ) : Require exact matching Examples : ``` a = { ' foo ' : ' bar ' } b = { ' foo ' : ' bar baz ' } match ( a , b ) # True a = { ' foo ' : ' bar ' } b = { ' foo ' : ' bar baz ' } match ( a , b , strict = True ) # False a = {} b = { ' foo ' : ' bar ' } match ( a , b ) # True a = {} b = {} match ( a , b ) # True ``` \"\"\" if not a : return True if not a and not b : return True if a and not b : return False for k , v in a . items () : if not b . get ( k ) : return False if strict : if v == b . get ( k ) : continue else : return False if v in b . get ( k ) : continue else : return False return True","title":"match"}]}